{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDdfFYEnUA4L",
        "outputId": "4b0358a2-2fd9-4a58-d6e7-884c9e4d98a1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install pytorch\n",
        "!pip install sklearn\n",
        "!pip install thop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y03TDf4aUITg",
        "outputId": "a730e8e7-d42f-482a-ed47-75d5f3d07b12"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m116.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n",
            "Collecting pytorch\n",
            "  Downloading pytorch-1.0.2.tar.gz (689 bytes)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pytorch\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for pytorch (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for pytorch\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for pytorch\n",
            "Failed to build pytorch\n",
            "\u001b[31mERROR: Could not build wheels for pytorch, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting sklearn\n",
            "  Downloading sklearn-0.0.post5.tar.gz (3.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: sklearn\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0.post5-py3-none-any.whl size=2950 sha256=ed5545069751237422deafd0b8e65c07dc284a37ffce693ec7488698558f83ac\n",
            "  Stored in directory: /root/.cache/pip/wheels/38/1f/8d/4f812c590e074c1e928f5cec67bf5053b71f38e2648739403a\n",
            "Successfully built sklearn\n",
            "Installing collected packages: sklearn\n",
            "Successfully installed sklearn-0.0.post5\n",
            "Collecting thop\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from thop) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->thop) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->thop) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->thop) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->thop) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->thop) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->thop) (1.3.0)\n",
            "Installing collected packages: thop\n",
            "Successfully installed thop-0.1.1.post2209072238\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Auk8ljxTwQCJ"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoModel\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self, embed_dim, journal_size):\n",
        "        super(MyModel, self).__init__()\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\",padding ='max_length',max_length = 512,truncation=True)\n",
        "        self.bert = AutoModel.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\")\n",
        "\n",
        "        # self.tokenizer = AutoTokenizer.from_pretrained(\"scibert_scivocab_uncased\")\n",
        "        self.atten = nn.MultiheadAttention(embed_dim=embed_dim,num_heads=8,dropout=0.1)\n",
        "        self.liner_query = nn.Linear(embed_dim, embed_dim)\n",
        "        self.liner_key = nn.Linear(embed_dim, embed_dim)\n",
        "        self.liner_value = nn.Linear(embed_dim, embed_dim)\n",
        "        self.liner1 = nn.Linear(journal_size, journal_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.liner2 = nn.Linear(journal_size+embed_dim, 2)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def get_sentence_feature(self, input_ids):\n",
        "        outputs = self.bert(input_ids)\n",
        "        pooled_output = outputs[1]\n",
        "        # print('pooled_output.shape:',pooled_output.shape)\n",
        "        return pooled_output\n",
        "\n",
        "    def forward(self, journal, title, abasruct):\n",
        "#         print('journal.shape',journal.shape)\n",
        "#         print('title.shape',title.shape)\n",
        "#         print('abasruct.shape',abasruct.shape)\n",
        "        title_vector = self.get_sentence_feature(title).unsqueeze(0)\n",
        "#         abasruct_vector = self.get_sentence_feature(abasruct).unsqueeze(0)\n",
        "#         print('title.shape',title_vector.shape)\n",
        "#         print('abasruct.shape',abasruct_vector.shape)\n",
        "        title_query_vector, title_key_vector, title_value_vector = self.liner_query(title_vector),self.liner_key(title_vector),self.liner_value(title_vector)\n",
        "        title_atten,_ = self.atten(title_query_vector, title_key_vector, title_value_vector)\n",
        "#         abasruct_query_vector, abasruct_key_vector, abasruct_value_vector = self.liner_query(abasruct_vector),self.liner_key(abasruct_vector),self.liner_value(abasruct_vector)\n",
        "#         abasruct_atten,_ = self.atten(abasruct_query_vector, abasruct_key_vector, abasruct_value_vector)\n",
        "        journal_vector = self.liner1(journal)\n",
        "        journal_vector = self.relu(journal_vector)\n",
        "        # print('journal_vector.shape',journal_vector.shape)\n",
        "#         print('title_atten.shape',title_atten.shape)\n",
        "#         print('abasruct_atten.shape',abasruct_atten.shape)\n",
        "        feature = torch.cat((journal_vector,title_atten.squeeze(0)), 1)\n",
        "        # feature = torch.cat((journal_vector, title_vector, abasruct_vector), 1)\n",
        "        out = self.liner2(feature)\n",
        "        output = self.softmax(out)\n",
        "        return output\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Rx3JgKEvF6X1"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "def focal_loss(\n",
        "    inputs: torch.Tensor,\n",
        "    targets: torch.Tensor,\n",
        "    alpha: float = 0.80,#0.40\n",
        "    gamma: float = 2,\n",
        "    reduction: str = \"mean\",\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        inputs: A float tensor of arbitrary shape.\n",
        "                The predictions which have been sigmod for each example.\n",
        "        targets: A float tensor with the same shape as inputs. Stores the binary\n",
        "                 classification label for each element in inputs\n",
        "                (0 for the negative class and 1 for the positive class).\n",
        "        alpha: (optional) Weighting factor in range (0,1) to balance\n",
        "                positive vs negative examples. Default = -1 (no weighting).\n",
        "        gamma: Exponent of the modulating factor (1 - p_t) to\n",
        "               balance easy vs hard examples.\n",
        "        reduction: 'none' | 'mean' | 'sum'\n",
        "                 'none': No reduction will be applied to the output.\n",
        "                 'mean': The output will be averaged.\n",
        "                 'sum': The output will be summed.\n",
        "    Returns:\n",
        "        Loss tensor with the reduction option applied.\n",
        "    \"\"\"\n",
        "    inputs = inputs.float()\n",
        "    targets = targets.float()\n",
        "    p = inputs\n",
        "    ce_loss = F.binary_cross_entropy(inputs, targets, reduction=\"none\")\n",
        "    p_t = p * targets + (1 - p) * (1 - targets)\n",
        "    loss = ce_loss * ((1 - p_t) ** gamma)\n",
        "\n",
        "    if alpha >= 0:\n",
        "        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n",
        "        loss = alpha_t * loss\n",
        "\n",
        "    if reduction == \"mean\":\n",
        "        loss = loss.mean()\n",
        "    elif reduction == \"sum\":\n",
        "        loss = loss.sum()\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHqo2jKkwPy6",
        "outputId": "fc3eee44-1905-436b-ee9d-c496ef257c40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i =  0\n",
            "loss =  0.06473059952259064\n",
            "i =  640\n",
            "loss =  0.055472131818532944\n",
            "i =  1280\n",
            "loss =  0.06012091785669327\n",
            "i =  1920\n",
            "loss =  0.045017652213573456\n",
            "i =  2560\n",
            "loss =  0.046386342495679855\n",
            "evaluating...\n",
            "epochs: 0\n",
            "PAJO-TJ AUC =  0.8473199036798947\n",
            "Specificity =  0.6268656716417911\n",
            "Sensitivity =  0.900497512437811\n",
            "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
            "[INFO] Register count_softmax() for <class 'torch.nn.modules.activation.Softmax'>.\n",
            "Flops: 130653152416.00\n",
            "MLOPS: 122.51\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9363    0.6269    0.7510       469\n",
            "           1     0.5084    0.9005    0.6499       201\n",
            "\n",
            "    accuracy                         0.7090       670\n",
            "   macro avg     0.7224    0.7637    0.7004       670\n",
            "weighted avg     0.8079    0.7090    0.7206       670\n",
            "\n",
            "i =  0\n",
            "loss =  0.04648805782198906\n",
            "i =  640\n",
            "loss =  0.048391420394182205\n",
            "i =  1280\n",
            "loss =  0.032119400799274445\n",
            "i =  1920\n",
            "loss =  0.024484550580382347\n",
            "i =  2560\n",
            "loss =  0.044599585235118866\n",
            "i =  0\n",
            "loss =  0.03951319307088852\n",
            "i =  640\n",
            "loss =  0.04825865477323532\n",
            "i =  1280\n",
            "loss =  0.04290011525154114\n",
            "i =  1920\n",
            "loss =  0.026469098404049873\n",
            "i =  2560\n",
            "loss =  0.02477521263062954\n",
            "i =  0\n",
            "loss =  0.03751898556947708\n",
            "i =  640\n",
            "loss =  0.016369938850402832\n",
            "i =  1280\n",
            "loss =  0.01753615029156208\n",
            "i =  1920\n",
            "loss =  0.02387247048318386\n",
            "i =  2560\n",
            "loss =  0.017645305022597313\n",
            "evaluating...\n",
            "epochs: 3\n",
            "PAJO-TJ AUC =  0.8613436018203227\n",
            "Specificity =  0.7356076759061834\n",
            "Sensitivity =  0.8308457711442786\n",
            "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
            "[INFO] Register count_softmax() for <class 'torch.nn.modules.activation.Softmax'>.\n",
            "Flops: 130653152416.00\n",
            "MLOPS: 120.39\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9103    0.7356    0.8137       469\n",
            "           1     0.5739    0.8308    0.6789       201\n",
            "\n",
            "    accuracy                         0.7642       670\n",
            "   macro avg     0.7421    0.7832    0.7463       670\n",
            "weighted avg     0.8094    0.7642    0.7732       670\n",
            "\n",
            "i =  0\n",
            "loss =  0.026206227019429207\n",
            "i =  640\n",
            "loss =  0.026611139997839928\n",
            "i =  1280\n",
            "loss =  0.03151370584964752\n",
            "i =  1920\n",
            "loss =  0.00946677103638649\n",
            "i =  2560\n",
            "loss =  0.0198284350335598\n",
            "i =  0\n",
            "loss =  0.012376101687550545\n",
            "i =  640\n",
            "loss =  0.016641994938254356\n",
            "i =  1280\n",
            "loss =  0.015970701351761818\n",
            "i =  1920\n",
            "loss =  0.004299316555261612\n",
            "i =  2560\n",
            "loss =  0.02116340771317482\n",
            "i =  0\n",
            "loss =  0.016979817301034927\n",
            "i =  640\n",
            "loss =  0.015987906605005264\n",
            "i =  1280\n",
            "loss =  0.013972481712698936\n",
            "i =  1920\n",
            "loss =  0.0037789284251630306\n",
            "i =  2560\n",
            "loss =  0.03703035041689873\n",
            "evaluating...\n",
            "epochs: 6\n",
            "PAJO-TJ AUC =  0.8617573115234063\n",
            "Specificity =  0.8400852878464818\n",
            "Sensitivity =  0.7114427860696517\n",
            "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
            "[INFO] Register count_softmax() for <class 'torch.nn.modules.activation.Softmax'>.\n",
            "Flops: 130653152416.00\n",
            "MLOPS: 119.06\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8717    0.8401    0.8556       469\n",
            "           1     0.6560    0.7114    0.6826       201\n",
            "\n",
            "    accuracy                         0.8015       670\n",
            "   macro avg     0.7638    0.7758    0.7691       670\n",
            "weighted avg     0.8070    0.8015    0.8037       670\n",
            "\n",
            "i =  0\n",
            "loss =  0.011221559718251228\n",
            "i =  640\n",
            "loss =  0.001076399814337492\n",
            "i =  1280\n",
            "loss =  0.002914415206760168\n",
            "i =  1920\n",
            "loss =  0.005503647495061159\n",
            "i =  2560\n",
            "loss =  0.0017495225183665752\n",
            "i =  0\n",
            "loss =  0.0015531949466094375\n",
            "i =  640\n",
            "loss =  0.016934406012296677\n",
            "i =  1280\n",
            "loss =  0.008025948889553547\n",
            "i =  1920\n",
            "loss =  0.004392040427774191\n",
            "i =  2560\n",
            "loss =  0.0019051814451813698\n",
            "i =  0\n",
            "loss =  0.002794289495795965\n",
            "i =  640\n",
            "loss =  0.0058721937239170074\n",
            "i =  1280\n",
            "loss =  0.001100985798984766\n",
            "i =  1920\n",
            "loss =  0.0021423213183879852\n",
            "i =  2560\n",
            "loss =  0.0012392648495733738\n",
            "evaluating...\n",
            "epochs: 9\n",
            "PAJO-TJ AUC =  0.8540135145169675\n",
            "Specificity =  0.7569296375266524\n",
            "Sensitivity =  0.8059701492537313\n",
            "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
            "[INFO] Register count_softmax() for <class 'torch.nn.modules.activation.Softmax'>.\n",
            "Flops: 130653152416.00\n",
            "MLOPS: 118.78\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9010    0.7569    0.8227       469\n",
            "           1     0.5870    0.8060    0.6792       201\n",
            "\n",
            "    accuracy                         0.7716       670\n",
            "   macro avg     0.7440    0.7814    0.7510       670\n",
            "weighted avg     0.8068    0.7716    0.7797       670\n",
            "\n",
            "i =  0\n",
            "loss =  0.0044283317402005196\n",
            "i =  640\n",
            "loss =  0.00040187512058764696\n",
            "i =  1280\n",
            "loss =  0.02158832550048828\n",
            "i =  1920\n",
            "loss =  0.00970793142914772\n",
            "i =  2560\n",
            "loss =  0.01594347506761551\n",
            "i =  0\n",
            "loss =  0.000527608091942966\n",
            "i =  640\n",
            "loss =  0.00038026852416805923\n",
            "i =  1280\n",
            "loss =  0.0004328872892074287\n",
            "i =  1920\n",
            "loss =  0.0006775514339096844\n",
            "i =  2560\n",
            "loss =  0.0006099121528677642\n",
            "i =  0\n",
            "loss =  0.004455592483282089\n",
            "i =  640\n",
            "loss =  1.6845542631926946e-05\n",
            "i =  1280\n",
            "loss =  0.00047057849587872624\n",
            "i =  1920\n",
            "loss =  0.02134503796696663\n",
            "i =  2560\n",
            "loss =  0.007260954473167658\n",
            "evaluating...\n",
            "epochs: 12\n",
            "PAJO-TJ AUC =  0.8395230669679322\n",
            "Specificity =  0.8486140724946695\n",
            "Sensitivity =  0.6368159203980099\n",
            "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
            "[INFO] Register count_softmax() for <class 'torch.nn.modules.activation.Softmax'>.\n",
            "Flops: 130653152416.00\n",
            "MLOPS: 119.04\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8450    0.8486    0.8468       469\n",
            "           1     0.6432    0.6368    0.6400       201\n",
            "\n",
            "    accuracy                         0.7851       670\n",
            "   macro avg     0.7441    0.7427    0.7434       670\n",
            "weighted avg     0.7845    0.7851    0.7848       670\n",
            "\n",
            "i =  0\n",
            "loss =  0.014380189590156078\n",
            "i =  640\n",
            "loss =  0.0015361057594418526\n",
            "i =  1280\n",
            "loss =  0.0014025988057255745\n",
            "i =  1920\n",
            "loss =  0.0006375139346346259\n",
            "i =  2560\n",
            "loss =  0.004019815940409899\n",
            "i =  0\n",
            "loss =  0.00035743004991672933\n",
            "i =  640\n",
            "loss =  6.57900600344874e-05\n",
            "i =  1280\n",
            "loss =  0.0001405126095050946\n",
            "i =  1920\n",
            "loss =  0.000249000295298174\n",
            "i =  2560\n",
            "loss =  0.000685580656863749\n",
            "i =  0\n",
            "loss =  0.00031170889269560575\n",
            "i =  640\n",
            "loss =  0.00014638937136624008\n",
            "i =  1280\n",
            "loss =  6.0410886362660676e-05\n",
            "i =  1920\n",
            "loss =  2.5041103071998805e-05\n",
            "i =  2560\n",
            "loss =  0.0008541693678125739\n",
            "evaluating...\n",
            "epochs: 15\n",
            "PAJO-TJ AUC =  0.8375924216868749\n",
            "Specificity =  0.8571428571428571\n",
            "Sensitivity =  0.6169154228855721\n",
            "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
            "[INFO] Register count_softmax() for <class 'torch.nn.modules.activation.Softmax'>.\n",
            "Flops: 130653152416.00\n",
            "MLOPS: 119.01\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8392    0.8571    0.8481       469\n",
            "           1     0.6492    0.6169    0.6327       201\n",
            "\n",
            "    accuracy                         0.7851       670\n",
            "   macro avg     0.7442    0.7370    0.7404       670\n",
            "weighted avg     0.7822    0.7851    0.7835       670\n",
            "\n",
            "i =  0\n",
            "loss =  0.001645587501116097\n",
            "i =  640\n",
            "loss =  2.873793891922105e-05\n",
            "i =  1280\n",
            "loss =  0.021173899993300438\n",
            "i =  1920\n",
            "loss =  0.012246971018612385\n",
            "i =  2560\n",
            "loss =  0.004346455447375774\n",
            "i =  0\n",
            "loss =  0.0011492392513900995\n",
            "i =  640\n",
            "loss =  0.003671950427815318\n",
            "i =  1280\n",
            "loss =  0.00036569376243278384\n",
            "i =  1920\n",
            "loss =  0.0018342395778745413\n",
            "i =  2560\n",
            "loss =  0.0004512868181336671\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from thop import profile\n",
        "import time\n",
        "\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "# device = 'cpu'\n",
        "\n",
        "batch_size = 32\n",
        "lr = 3e-5\n",
        "EPOCHS = 18\n",
        "\n",
        "def shuffle_dataset(journal, title, abstruct, label):\n",
        "    length = len(journal)\n",
        "    rng = np.random.default_rng(12345)\n",
        "    index = np.arange(length)\n",
        "    # print(index)\n",
        "    rng.shuffle(index)\n",
        "    # print(index)\n",
        "    return journal[index], title[index], abstruct[index], label[index]\n",
        "\n",
        "def evaluate(model, test_journal, test_title, test_abstruct, test_Y):\n",
        "    model.eval()\n",
        "    pred = []\n",
        "    logits = []\n",
        "    true_Y = []\n",
        "    length = len(test_Y)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(test_Y), batch_size):\n",
        "        # for i in range(0, 10, batch_size):\n",
        "            logit = model(test_journal[i:min(len(test_Y),i+batch_size)], test_title[i:min(len(test_Y),i+batch_size)], test_abstruct[i:min(len(test_Y),i+batch_size)]).cpu()\n",
        "            logits.extend(logit)\n",
        "            y_pred = torch.argmax(logit, dim=1).cpu()\n",
        "            pred.extend(y_pred)\n",
        "            true_Y.extend(test_Y[i:min(len(test_Y),i+batch_size)].cpu())\n",
        "        # print(true_Y)\n",
        "        # print(logits)\n",
        "        # print(pred)\n",
        "        true_label = []\n",
        "        prob = []\n",
        "        pred_label = []\n",
        "        for i in range(len(true_Y)):\n",
        "          true_label.append(true_Y[i].item())\n",
        "          prob.append(logits[i][1].item())\n",
        "          pred_label.append(pred[i].item())\n",
        "        # print(true_label)\n",
        "        # print(prob)\n",
        "        # print(pred_label)\n",
        "\n",
        "        # 计算specificity、sensitivity\n",
        "        tn, fp, fn, tp = confusion_matrix(true_label, pred_label).ravel()\n",
        "        specificity = tn / (tn + fp)\n",
        "        sensitivity = tp / (tp + fn)\n",
        "\n",
        "    return {\n",
        "        'label':true_label, 'proba':prob,\n",
        "        'AUC':roc_auc_score(true_label, prob),\n",
        "        'classification_report':classification_report(true_label, pred_label, digits=4),\n",
        "        'specificity': specificity,\n",
        "        'sensitivity': sensitivity\n",
        "    }\n",
        "\n",
        "\n",
        "def main():\n",
        "    # 按0和1生成训练集和测试集\n",
        "    X_0 = pd.read_excel('/content/drive/MyDrive/pajo_data/X_0.xlsx').values\n",
        "    Y_0 = pd.read_excel('/content/drive/MyDrive/pajo_data/Y_0.xlsx').values\n",
        "    X_1 = pd.read_excel('/content/drive/MyDrive/pajo_data/X_1.xlsx').values\n",
        "    Y_1 = pd.read_excel('/content/drive/MyDrive/pajo_data/Y_1.xlsx').values\n",
        "    title_0 = np.load('/content/drive/MyDrive/pajo_data/token_title_0.npy')\n",
        "    title_1 = np.load('/content/drive/MyDrive/pajo_data/token_title_1.npy')\n",
        "    abstruct_0 = np.load('/content/drive/MyDrive/pajo_data/token_abstruct_0_512.npy')\n",
        "    abstruct_1 = np.load('/content/drive/MyDrive/pajo_data/token_abstruct_1_512.npy')\n",
        "\n",
        "    rs = np.random.RandomState(42)\n",
        "    L = list(rs.randint(0, len(X_0), int(7/3*len(X_1))))\n",
        "    X_0 = X_0[L]\n",
        "    Y_0 = Y_0[L]\n",
        "    title_0=title_0[L]\n",
        "    abstruct_0 = abstruct_0[L]\n",
        "\n",
        "    journal_train_X_0, journal_test_X_0, train_Y_0,test_Y_0 = train_test_split(X_0, Y_0, train_size=0.80, random_state=42)\n",
        "    journal_train_X_1, journal_test_X_1, train_Y_1, test_Y_1 = train_test_split(X_1, Y_1, train_size=0.80, random_state=42)\n",
        "    title_train_X_0, title_test_X_0, _,_ = train_test_split(title_0, Y_0, train_size=0.80, random_state=42)\n",
        "    title_train_X_1, title_test_X_1, _,_ = train_test_split(title_1, Y_1, train_size=0.80, random_state=42)\n",
        "    abstruct_train_X_0, abstruct_test_X_0, _,_ = train_test_split(abstruct_0, Y_0, train_size=0.80, random_state=42)\n",
        "    abstruct_train_X_1, abstruct_test_X_1, _,_ = train_test_split(abstruct_1, Y_1, train_size=0.80, random_state=42)\n",
        "\n",
        "    test_journal = torch.from_numpy(np.vstack((journal_test_X_1, journal_test_X_0))).float().to(device)\n",
        "    test_title = torch.from_numpy(np.vstack((title_test_X_1, title_test_X_0))).to(device)\n",
        "    test_abstruct = torch.from_numpy(np.vstack((abstruct_test_X_1, abstruct_test_X_0))).to(device)\n",
        "    test_Y = torch.from_numpy(np.vstack((test_Y_1, test_Y_0))).to(device)\n",
        "    test_journal, test_title, test_abstruct, test_Y = shuffle_dataset(test_journal, test_title, test_abstruct, test_Y)\n",
        "\n",
        "\n",
        "    model = MyModel(embed_dim=768, journal_size=test_journal.shape[1]).to(device)\n",
        "#     model.load_state_dict(torch.load(\"./res/new_model_2_0.75.pt\"))\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "    journal_train_X_0 = torch.from_numpy(journal_train_X_0).float().to(device)\n",
        "    title_train_X_0 = torch.from_numpy(title_train_X_0).to(device)\n",
        "    abstruct_train_X_0 = torch.from_numpy(abstruct_train_X_0).to(device)\n",
        "    train_Y_0 = torch.from_numpy(train_Y_0).to(device)\n",
        "\n",
        "    journal_train_X_1 = torch.from_numpy(journal_train_X_1).float().float().to(device)\n",
        "    title_train_X_1 = torch.from_numpy(title_train_X_1).to(device)\n",
        "    abstruct_train_X_1 = torch.from_numpy(abstruct_train_X_1).to(device)\n",
        "    train_Y_1 = torch.from_numpy(train_Y_1).to(device)\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        start_time = time.time()\n",
        "        journal_train_X_0, title_train_X_0, abstruct_train_X_0, train_Y_0 = shuffle_dataset(journal_train_X_0, title_train_X_0, abstruct_train_X_0, train_Y_0)\n",
        "        train_journal = torch.cat((journal_train_X_1, journal_train_X_0))\n",
        "        # print('journal_train_X_1.shape:',journal_train_X_1.shape)\n",
        "        train_title = torch.cat((title_train_X_1, title_train_X_0))\n",
        "        train_abstruct = torch.cat((abstruct_train_X_1, abstruct_train_X_0))\n",
        "        train_Y = torch.cat((train_Y_1, train_Y_0))\n",
        "        train_journal, train_title, train_abstruct, train_Y = shuffle_dataset(train_journal, train_title, train_abstruct, train_Y)\n",
        "\n",
        "        for i in range(0, len(train_Y), batch_size):\n",
        "        # for i in range(0, 10, batch_size):\n",
        "            model.zero_grad()\n",
        "            logits = model(train_journal[i:min(len(train_Y),i+batch_size)], train_title[i:min(len(train_Y),i+batch_size)], train_abstruct[i:min(len(train_Y),i+batch_size)])\n",
        "            loss = focal_loss(logits[:,1].unsqueeze(1), train_Y[i:i+batch_size])\n",
        "            loss.backward()\n",
        "            if((i/batch_size)%20 == 0):\n",
        "              print('i = ', i)\n",
        "              print('loss = ', loss.item())\n",
        "            optimizer.step()\n",
        "\n",
        "        end_time = time.time()\n",
        "\n",
        "        # 计算每秒钟的训练样本数（即MLOPS）\n",
        "        num_samples = len(train_Y)\n",
        "        training_time = end_time - start_time\n",
        "        samples_per_second = num_samples / training_time\n",
        "\n",
        "        if(epoch%3 == 0):\n",
        "\n",
        "              print('evaluating...')\n",
        "\n",
        "              val_metrics = evaluate(model, test_journal, test_title, test_abstruct, test_Y)\n",
        "\n",
        "              print('epochs:',epoch)\n",
        "              print('PAJO-TJ AUC = ',val_metrics['AUC'])\n",
        "              print('Specificity = ', val_metrics['specificity'])\n",
        "              print('Sensitivity = ', val_metrics['sensitivity'])\n",
        "              # 输出模型信息\n",
        "              flops, params = profile(model, inputs=(train_journal[0:batch_size], train_title[0:batch_size], train_abstruct[0:batch_size]))\n",
        "              print(\"Flops: {:.2f}\".format(flops))\n",
        "              print(\"MLOPS: {:.2f}\".format(samples_per_second))\n",
        "              print(val_metrics['classification_report'])\n",
        "\n",
        "              torch.save(model.state_dict(), './res/no_key_title_jou_pumbed_2_0.80_{}.pt'.format(epoch))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    if not os.path.exists('./res'):\n",
        "        os.mkdir('./res')\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REe-ocOQT-tD"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings(action = 'ignore')\n",
        "\n",
        "def best_yuzhi_aimed_at_1(preda,y_test):\n",
        "    #preda为预测为1类的概率,输入形式为narray\n",
        "    precisions = []\n",
        "    recalls = []\n",
        "    for i in np.arange(0.01,1,0.01):\n",
        "        y_pred = np.where(preda<i,0,1)\n",
        "        TN,FP,FN,TP = metrics.confusion_matrix(y_test,y_pred).ravel()\n",
        "        precision = TP/(TP+FP)\n",
        "        recall = TP/(TP+FN)\n",
        "        precisions.append(precision)\n",
        "        recalls.append(recall)\n",
        "    precisions = np.array(precisions)\n",
        "    recalls = np.array(recalls)\n",
        "    f1_scores = (2 * precisions * recalls) / (precisions + recalls)\n",
        "    best_f1_score = np.max(f1_scores[np.isfinite(f1_scores)])\n",
        "    best_f1_score_index = np.argmax(f1_scores[np.isfinite(f1_scores)])\n",
        "    return best_f1_score, np.arange(0.01,1,0.01)[best_f1_score_index]\n",
        "#调用示例\n",
        "y_test=np.array([0,0,1,1,1,0,1,0,1,0,1,1])\n",
        "preda = np.array([0.2,0.3,0.4,0.44,0.45,0.56,0.3,0.1,0.7,0.9,0.13,0.5])\n",
        "print(best_yuzhi_aimed_at_1(preda,y_test))\n",
        "#输出\n",
        "(0.7777777777777778, 0.11)#（1类最佳f1值和对应的阈值）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2n-mWc2AT-tD"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "def plot_picture(y_test,probas):\n",
        "    # y_test测试集\n",
        "    # probas预测概率\n",
        "    CVD = pd.DataFrame()\n",
        "    CVD['正样本'] = y_test\n",
        "    CVD['score'] = probas\n",
        "    CVD = CVD.sort_values(by='score')\n",
        "    cvd_risk = CVD.reset_index(drop=True)\n",
        "    print(cvd_risk)\n",
        "    H = len(cvd_risk)\n",
        "    h = int(H / 10)\n",
        "    cvd = []\n",
        "    risk = []\n",
        "    h0 = 0\n",
        "    risk_count = 0\n",
        "    cvd_count = 0\n",
        "    for i in range(len(cvd_risk)):\n",
        "        if h0 + h > i + 1:\n",
        "            risk_count = risk_count + cvd_risk.loc[i, \"score\"]\n",
        "            if cvd_risk.loc[i, \"正样本\"] == 1:\n",
        "                cvd_count = cvd_count + 1\n",
        "        else:\n",
        "            h0 = h0 + h\n",
        "            cvd.append(round(cvd_count / h, 3))\n",
        "            risk.append(round(risk_count / h, 3))\n",
        "            risk_count = 0\n",
        "            cvd_count = 0\n",
        "\n",
        "    labels = ['10', '9', '8', '7', '6', '5', '4', '3', '2', '1']\n",
        "    cvd.reverse()\n",
        "\n",
        "    x = np.arange(len(labels))  # the label locations\n",
        "    width = 0.8  # the width of the bars\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    rects1 = ax.bar(x, cvd, width, color='royalblue')\n",
        "    # rects2 = ax.bar(x + width / 2, risk, width, label='Estimated', color='indianred')\n",
        "\n",
        "    # Add some text for labels, title and custom x-axis tick labels, etc.\n",
        "    ax.set_ylabel('Proportion of Positive Samples')\n",
        "    ax.set_xlabel('Decile of Estimated Score')\n",
        "    # ax.set_title('Observed Vs Estimated')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(labels)\n",
        "    ax.legend()\n",
        "    plt.axhline(y=np.mean(cvd),  linestyle='--', color='black')\n",
        "    def autolabel(rects):\n",
        "        \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
        "        for rect in rects:\n",
        "            height = rect.get_height()\n",
        "            ax.annotate('{}'.format(height),\n",
        "                        xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                        xytext=(0, 2),  # 3 points vertical offset\n",
        "                        textcoords=\"offset points\",\n",
        "                        ha='center', va='bottom')\n",
        "\n",
        "    autolabel(rects1)\n",
        "    # autolabel(rects2)\n",
        "    fig.tight_layout()\n",
        "    plt.savefig(\"柱状图.png\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cATUN3ET-tD"
      },
      "outputs": [],
      "source": [
        "def yuzhi(preda,door=0.1):\n",
        "    predict=[]\n",
        "    for i in range(len(preda)):\n",
        "        if preda[i] < door:\n",
        "            predict.append(0)\n",
        "        else:\n",
        "            predict.append(1)\n",
        "    return predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbLA4Q4kT-tD"
      },
      "outputs": [],
      "source": [
        "#按阈值最大\n",
        "from sklearn.metrics import roc_auc_score,roc_curve,auc\n",
        "from numpy import argmax\n",
        "def find_optimal_cutoff(tpr,fpr,threshold):\n",
        "    optimal_idx = np.argmax(tpr - fpr)\n",
        "    optimal_threshold = threshold[optimal_idx]\n",
        "    return optimal_threshold\n",
        "\n",
        "def best_confusion_matrix(y_test, y_test_predprob):\n",
        "    \"\"\"\n",
        "        根据真实值和预测值（预测概率）的向量来计算混淆矩阵和最优的划分阈值\n",
        "\n",
        "        Args:\n",
        "            y_test:真实值\n",
        "            y_test_predprob：预测值\n",
        "\n",
        "        Returns:\n",
        "            返回最佳划分阈值和混淆矩阵\n",
        "        \"\"\"\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, y_test_predprob, pos_label=1)\n",
        "    cutoff = find_optimal_cutoff(tpr,fpr,thresholds)\n",
        "    y_pred = yuzhi(y_test_predprob,cutoff)\n",
        "    print(metrics.confusion_matrix(y_true=y_test, y_pred=y_pred))\n",
        "    print(metrics.classification_report(y_true=y_test, y_pred=y_pred))\n",
        "    TN,FP,FN,TP = metrics.confusion_matrix(y_test,y_pred).ravel()\n",
        "    return cutoff,TN,FN,FP,TP\n",
        "best_confusion_matrix(test_Y_input,preda)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhnxOI3sT-tE"
      },
      "outputs": [],
      "source": [
        "precisions, recalls, thresholds = precision_recall_curve(test_Y_input,predict)\n",
        "\n",
        "# 拿到最优结果以及索引\n",
        "f1_scores = (2 * precisions * recalls) / (precisions + recalls)\n",
        "best_f1_score = np.max(f1_scores[np.isfinite(f1_scores)])\n",
        "best_f1_score_index = np.argmax(f1_scores[np.isfinite(f1_scores)])\n",
        "\n",
        "# 阈值\n",
        "best_f1_score, thresholds[best_f1_score_index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-4S-nk4w61b",
        "outputId": "b72699f0-3c33-4c5e-df9a-a37e7b051c80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  data.zip\n",
            "   creating: data/\n",
            "  inflating: data/token_abstruct_0_400.npy  \n",
            "  inflating: data/token_abstruct_1_400.npy  \n",
            "  inflating: data/token_title_0.npy  \n",
            "  inflating: data/token_title_1.npy  \n",
            "  inflating: data/X_0.xlsx           \n",
            "  inflating: data/X_1.xlsx           \n",
            "  inflating: data/Y_0.xlsx           \n",
            "  inflating: data/Y_1.xlsx           \n",
            "   creating: data/__pycache__/\n",
            "  inflating: data/__pycache__/dataset.cpython-37.pyc  \n"
          ]
        }
      ],
      "source": [
        "!unzip data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qw7zyYoiZiaI",
        "outputId": "49f393f2-2f40-4461-ffd7-5a869e7a6b79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch_metric_learning\n",
            "  Downloading pytorch_metric_learning-1.5.2-py3-none-any.whl (111 kB)\n",
            "\u001b[K     |████████████████████████████████| 111 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_metric_learning) (4.64.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_metric_learning) (1.21.6)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from pytorch_metric_learning) (0.13.1+cu113)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_metric_learning) (1.12.1+cu113)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pytorch_metric_learning) (1.0.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (4.1.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pytorch_metric_learning) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pytorch_metric_learning) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pytorch_metric_learning) (1.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->pytorch_metric_learning) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->pytorch_metric_learning) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->pytorch_metric_learning) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->pytorch_metric_learning) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->pytorch_metric_learning) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->pytorch_metric_learning) (3.0.4)\n",
            "Installing collected packages: pytorch-metric-learning\n",
            "Successfully installed pytorch-metric-learning-1.5.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch_metric_learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0nsOF6V8Vf3",
        "outputId": "e93848d5-fa12-4bd9-8d8a-32241c5fe381"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.21.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqYGm62_8a4-",
        "outputId": "d01f4b10-31bb-4d30-f002-b7c1c59424c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.21.1-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 5.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 47.9 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 47.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 12.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.21.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZ4E0YDZZXh1",
        "outputId": "257db48b-6628-40a4-d2fd-dc09b3cf881c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  data.zip\n",
            "   creating: data/\n",
            "  inflating: data/token_abstruct_0_400.npy  \n",
            "  inflating: data/token_abstruct_1_400.npy  \n",
            "  inflating: data/token_title_0.npy  \n",
            "  inflating: data/token_title_1.npy  \n",
            "  inflating: data/X_0.xlsx           \n",
            "  inflating: data/X_1.xlsx           \n",
            "  inflating: data/Y_0.xlsx           \n",
            "  inflating: data/Y_1.xlsx           \n",
            "   creating: data/__pycache__/\n",
            "  inflating: data/__pycache__/dataset.cpython-37.pyc  \n"
          ]
        }
      ],
      "source": [
        "!unzip data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUwm114yjvuD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}