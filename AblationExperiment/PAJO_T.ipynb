{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-Jr8aJlb-fA",
        "outputId": "3d3aeccb-88ff-4cdc-92be-31c59feb394f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install pytorch\n",
        "!pip install sklearn\n",
        "!pip install thop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-8nDI6-cBIg",
        "outputId": "8896cf4f-4cfe-4688-b6fd-faf10d7d877e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/7.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/7.2 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m5.6/7.2 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n",
            "Collecting pytorch\n",
            "  Downloading pytorch-1.0.2.tar.gz (689 bytes)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pytorch\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for pytorch (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for pytorch\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for pytorch\n",
            "Failed to build pytorch\n",
            "\u001b[31mERROR: Could not build wheels for pytorch, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting sklearn\n",
            "  Downloading sklearn-0.0.post5.tar.gz (3.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: sklearn\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0.post5-py3-none-any.whl size=2950 sha256=2a2d62b00863e2233b4eccec22e5eda175545466940a8c1a41152cc4d0c638f7\n",
            "  Stored in directory: /root/.cache/pip/wheels/38/1f/8d/4f812c590e074c1e928f5cec67bf5053b71f38e2648739403a\n",
            "Successfully built sklearn\n",
            "Installing collected packages: sklearn\n",
            "Successfully installed sklearn-0.0.post5\n",
            "Collecting thop\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from thop) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->thop) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->thop) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->thop) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->thop) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->thop) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->thop) (1.3.0)\n",
            "Installing collected packages: thop\n",
            "Successfully installed thop-0.1.1.post2209072238\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Auk8ljxTwQCJ"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoModel\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self, embed_dim, journal_size):\n",
        "        super(MyModel, self).__init__()\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\",padding ='max_length',max_length = 512,truncation=True)\n",
        "        self.bert = AutoModel.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\")\n",
        "\n",
        "        # self.tokenizer = AutoTokenizer.from_pretrained(\"scibert_scivocab_uncased\")\n",
        "        self.atten = nn.MultiheadAttention(embed_dim=embed_dim,num_heads=8,dropout=0.1)\n",
        "        self.liner_query = nn.Linear(embed_dim, embed_dim)\n",
        "        self.liner_key = nn.Linear(embed_dim, embed_dim)\n",
        "        self.liner_value = nn.Linear(embed_dim, embed_dim)\n",
        "        self.liner1 = nn.Linear(journal_size, journal_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.liner2 = nn.Linear(embed_dim, 2)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def get_sentence_feature(self, input_ids):\n",
        "        outputs = self.bert(input_ids)\n",
        "        pooled_output = outputs[1]\n",
        "        # print('pooled_output.shape:',pooled_output.shape)\n",
        "        return pooled_output\n",
        "\n",
        "    def forward(self, journal, title, abasruct):\n",
        "#         print('journal.shape',journal.shape)\n",
        "#         print('title.shape',title.shape)\n",
        "#         print('abasruct.shape',abasruct.shape)\n",
        "        title_vector = self.get_sentence_feature(title).unsqueeze(0)\n",
        "#         abasruct_vector = self.get_sentence_feature(abasruct).unsqueeze(0)\n",
        "#         print('title.shape',title_vector.shape)\n",
        "#         print('abasruct.shape',abasruct_vector.shape)\n",
        "        title_query_vector, title_key_vector, title_value_vector = self.liner_query(title_vector),self.liner_key(title_vector),self.liner_value(title_vector)\n",
        "        title_atten,_ = self.atten(title_query_vector, title_key_vector, title_value_vector)\n",
        "#         abasruct_query_vector, abasruct_key_vector, abasruct_value_vector = self.liner_query(abasruct_vector),self.liner_key(abasruct_vector),self.liner_value(abasruct_vector)\n",
        "#         abasruct_atten,_ = self.atten(abasruct_query_vector, abasruct_key_vector, abasruct_value_vector)\n",
        "#         journal_vector = self.liner1(journal)\n",
        "#         journal_vector = self.relu(journal_vector)\n",
        "        # print('journal_vector.shape',journal_vector.shape)\n",
        "#         print('title_atten.shape',title_atten.shape)\n",
        "#         print('abasruct_atten.shape',abasruct_atten.shape)\n",
        "#         feature = torch.cat((journal_vector, title_atten.squeeze(0), abasruct_atten.squeeze(0)), 1)\n",
        "        # feature = torch.cat((journal_vector, title_vector, abasruct_vector), 1)\n",
        "        out = self.liner2(title_atten.squeeze(0))\n",
        "        output = self.softmax(out)\n",
        "        return output\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rx3JgKEvF6X1"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "def focal_loss(\n",
        "    inputs: torch.Tensor,\n",
        "    targets: torch.Tensor,\n",
        "    alpha: float = 0.80,#0.40\n",
        "    gamma: float = 2,\n",
        "    reduction: str = \"mean\",\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        inputs: A float tensor of arbitrary shape.\n",
        "                The predictions which have been sigmod for each example.\n",
        "        targets: A float tensor with the same shape as inputs. Stores the binary\n",
        "                 classification label for each element in inputs\n",
        "                (0 for the negative class and 1 for the positive class).\n",
        "        alpha: (optional) Weighting factor in range (0,1) to balance\n",
        "                positive vs negative examples. Default = -1 (no weighting).\n",
        "        gamma: Exponent of the modulating factor (1 - p_t) to\n",
        "               balance easy vs hard examples.\n",
        "        reduction: 'none' | 'mean' | 'sum'\n",
        "                 'none': No reduction will be applied to the output.\n",
        "                 'mean': The output will be averaged.\n",
        "                 'sum': The output will be summed.\n",
        "    Returns:\n",
        "        Loss tensor with the reduction option applied.\n",
        "    \"\"\"\n",
        "    inputs = inputs.float()\n",
        "    targets = targets.float()\n",
        "    p = inputs\n",
        "    ce_loss = F.binary_cross_entropy(inputs, targets, reduction=\"none\")\n",
        "    p_t = p * targets + (1 - p) * (1 - targets)\n",
        "    loss = ce_loss * ((1 - p_t) ** gamma)\n",
        "\n",
        "    if alpha >= 0:\n",
        "        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n",
        "        loss = alpha_t * loss\n",
        "\n",
        "    if reduction == \"mean\":\n",
        "        loss = loss.mean()\n",
        "    elif reduction == \"sum\":\n",
        "        loss = loss.sum()\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHqo2jKkwPy6",
        "outputId": "50183d65-9ef8-400a-8c94-8b4949d5f2ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i =  0\n",
            "loss =  0.06419514119625092\n",
            "i =  640\n",
            "loss =  0.052350983023643494\n",
            "i =  1280\n",
            "loss =  0.05730826407670975\n",
            "i =  1920\n",
            "loss =  0.04826055467128754\n",
            "i =  2560\n",
            "loss =  0.047575365751981735\n",
            "evaluating...\n",
            "epochs: 0\n",
            "PAJO-T AUC =  0.8468849780946017\n",
            "Specificity =  0.4690831556503198\n",
            "Sensitivity =  0.9502487562189055\n",
            "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
            "[INFO] Register count_softmax() for <class 'torch.nn.modules.activation.Softmax'>.\n",
            "Flops: 130653143200.00\n",
            "MLOPS: 113.09\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9565    0.4691    0.6295       469\n",
            "           1     0.4341    0.9502    0.5959       201\n",
            "\n",
            "    accuracy                         0.6134       670\n",
            "   macro avg     0.6953    0.7097    0.6127       670\n",
            "weighted avg     0.7998    0.6134    0.6194       670\n",
            "\n",
            "i =  0\n",
            "loss =  0.04492881894111633\n",
            "i =  640\n",
            "loss =  0.03697808459401131\n",
            "i =  1280\n",
            "loss =  0.03947078436613083\n",
            "i =  1920\n",
            "loss =  0.030870316550135612\n",
            "i =  2560\n",
            "loss =  0.04811151325702667\n",
            "i =  0\n",
            "loss =  0.03805268183350563\n",
            "i =  640\n",
            "loss =  0.037934526801109314\n",
            "i =  1280\n",
            "loss =  0.04529538005590439\n",
            "i =  1920\n",
            "loss =  0.027925193309783936\n",
            "i =  2560\n",
            "loss =  0.03194933384656906\n",
            "i =  0\n",
            "loss =  0.03712695091962814\n",
            "i =  640\n",
            "loss =  0.013333464041352272\n",
            "i =  1280\n",
            "loss =  0.014490578323602676\n",
            "i =  1920\n",
            "loss =  0.02044147253036499\n",
            "i =  2560\n",
            "loss =  0.014073546975851059\n",
            "evaluating...\n",
            "epochs: 3\n",
            "PAJO-T AUC =  0.8748899426110386\n",
            "Specificity =  0.6929637526652452\n",
            "Sensitivity =  0.8855721393034826\n",
            "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
            "[INFO] Register count_softmax() for <class 'torch.nn.modules.activation.Softmax'>.\n",
            "Flops: 130653143200.00\n",
            "MLOPS: 109.01\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9339    0.6930    0.7956       469\n",
            "           1     0.5528    0.8856    0.6807       201\n",
            "\n",
            "    accuracy                         0.7507       670\n",
            "   macro avg     0.7434    0.7893    0.7381       670\n",
            "weighted avg     0.8196    0.7507    0.7611       670\n",
            "\n",
            "i =  0\n",
            "loss =  0.031476154923439026\n",
            "i =  640\n",
            "loss =  0.013963762670755386\n",
            "i =  1280\n",
            "loss =  0.030202310532331467\n",
            "i =  1920\n",
            "loss =  0.010934984311461449\n",
            "i =  2560\n",
            "loss =  0.017726361751556396\n",
            "i =  0\n",
            "loss =  0.006021887995302677\n",
            "i =  640\n",
            "loss =  0.018632959574460983\n",
            "i =  1280\n",
            "loss =  0.0036084174644201994\n",
            "i =  1920\n",
            "loss =  0.0026833144947886467\n",
            "i =  2560\n",
            "loss =  0.005560880061239004\n",
            "i =  0\n",
            "loss =  0.007097774185240269\n",
            "i =  640\n",
            "loss =  0.001184085733257234\n",
            "i =  1280\n",
            "loss =  0.0030410487670451403\n",
            "i =  1920\n",
            "loss =  0.003734852187335491\n",
            "i =  2560\n",
            "loss =  0.008661720901727676\n",
            "evaluating...\n",
            "epochs: 6\n",
            "PAJO-T AUC =  0.8652155003235422\n",
            "Specificity =  0.908315565031983\n",
            "Sensitivity =  0.6318407960199005\n",
            "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
            "[INFO] Register count_softmax() for <class 'torch.nn.modules.activation.Softmax'>.\n",
            "Flops: 130653143200.00\n",
            "MLOPS: 107.71\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8520    0.9083    0.8793       469\n",
            "           1     0.7471    0.6318    0.6846       201\n",
            "\n",
            "    accuracy                         0.8254       670\n",
            "   macro avg     0.7995    0.7701    0.7819       670\n",
            "weighted avg     0.8205    0.8254    0.8209       670\n",
            "\n",
            "i =  0\n",
            "loss =  0.01393833290785551\n",
            "i =  640\n",
            "loss =  0.011531494557857513\n",
            "i =  1280\n",
            "loss =  0.0023088741581887007\n",
            "i =  1920\n",
            "loss =  0.005655493587255478\n",
            "i =  2560\n",
            "loss =  0.004335551988333464\n",
            "i =  0\n",
            "loss =  0.00399351678788662\n",
            "i =  640\n",
            "loss =  0.0007016409654170275\n",
            "i =  1280\n",
            "loss =  0.0007411298574879766\n",
            "i =  1920\n",
            "loss =  0.002024260349571705\n",
            "i =  2560\n",
            "loss =  0.017369752749800682\n",
            "i =  0\n",
            "loss =  0.0023353397846221924\n",
            "i =  640\n",
            "loss =  0.00027495427639223635\n",
            "i =  1280\n",
            "loss =  0.0006927084177732468\n",
            "i =  1920\n",
            "loss =  0.0015364181017503142\n",
            "i =  2560\n",
            "loss =  0.006540423724800348\n",
            "evaluating...\n",
            "epochs: 9\n",
            "PAJO-T AUC =  0.8671991853101232\n",
            "Specificity =  0.8315565031982942\n",
            "Sensitivity =  0.7562189054726368\n",
            "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
            "[INFO] Register count_softmax() for <class 'torch.nn.modules.activation.Softmax'>.\n",
            "Flops: 130653143200.00\n",
            "MLOPS: 107.89\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8884    0.8316    0.8590       469\n",
            "           1     0.6580    0.7562    0.7037       201\n",
            "\n",
            "    accuracy                         0.8090       670\n",
            "   macro avg     0.7732    0.7939    0.7814       670\n",
            "weighted avg     0.8193    0.8090    0.8124       670\n",
            "\n",
            "i =  0\n",
            "loss =  0.0009207009570673108\n",
            "i =  640\n",
            "loss =  0.00010614484926918522\n",
            "i =  1280\n",
            "loss =  0.01118900440633297\n",
            "i =  1920\n",
            "loss =  0.0008527961908839643\n",
            "i =  2560\n",
            "loss =  0.010792204178869724\n",
            "i =  0\n",
            "loss =  0.00089151511201635\n",
            "i =  640\n",
            "loss =  0.0017805733950808644\n",
            "i =  1280\n",
            "loss =  0.0007707866025157273\n",
            "i =  1920\n",
            "loss =  0.01307919342070818\n",
            "i =  2560\n",
            "loss =  0.002021780237555504\n",
            "i =  0\n",
            "loss =  0.0034517061430960894\n",
            "i =  640\n",
            "loss =  0.000470160273835063\n",
            "i =  1280\n",
            "loss =  0.0008032395853661001\n",
            "i =  1920\n",
            "loss =  0.0038768877275288105\n",
            "i =  2560\n",
            "loss =  0.0006822907598689198\n",
            "evaluating...\n",
            "epochs: 12\n",
            "PAJO-T AUC =  0.8567715792041922\n",
            "Specificity =  0.8230277185501066\n",
            "Sensitivity =  0.736318407960199\n",
            "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
            "[INFO] Register count_softmax() for <class 'torch.nn.modules.activation.Softmax'>.\n",
            "Flops: 130653143200.00\n",
            "MLOPS: 107.65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8793    0.8230    0.8502       469\n",
            "           1     0.6407    0.7363    0.6852       201\n",
            "\n",
            "    accuracy                         0.7970       670\n",
            "   macro avg     0.7600    0.7797    0.7677       670\n",
            "weighted avg     0.8077    0.7970    0.8007       670\n",
            "\n",
            "i =  0\n",
            "loss =  0.0009248846326954663\n",
            "i =  640\n",
            "loss =  0.0029177775140851736\n",
            "i =  1280\n",
            "loss =  0.0005915346555411816\n",
            "i =  1920\n",
            "loss =  0.0066859787330031395\n",
            "i =  2560\n",
            "loss =  0.000788346806075424\n",
            "i =  0\n",
            "loss =  0.020625727251172066\n",
            "i =  640\n",
            "loss =  0.00038018994382582605\n",
            "i =  1280\n",
            "loss =  0.00013128487626090646\n",
            "i =  1920\n",
            "loss =  0.0006453200476244092\n",
            "i =  2560\n",
            "loss =  0.0008776543545536697\n",
            "i =  0\n",
            "loss =  0.0005932335625402629\n",
            "i =  640\n",
            "loss =  0.0005186760099604726\n",
            "i =  1280\n",
            "loss =  6.0435519117163494e-05\n",
            "i =  1920\n",
            "loss =  0.00014698992890771478\n",
            "i =  2560\n",
            "loss =  3.0865361623000354e-05\n",
            "evaluating...\n",
            "epochs: 15\n",
            "PAJO-T AUC =  0.8615239368191028\n",
            "Specificity =  0.8592750533049041\n",
            "Sensitivity =  0.6766169154228856\n",
            "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
            "[INFO] Register count_softmax() for <class 'torch.nn.modules.activation.Softmax'>.\n",
            "Flops: 130653143200.00\n",
            "MLOPS: 107.71\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8611    0.8593    0.8602       469\n",
            "           1     0.6733    0.6766    0.6749       201\n",
            "\n",
            "    accuracy                         0.8045       670\n",
            "   macro avg     0.7672    0.7679    0.7676       670\n",
            "weighted avg     0.8048    0.8045    0.8046       670\n",
            "\n",
            "i =  0\n",
            "loss =  4.9360867706127465e-05\n",
            "i =  640\n",
            "loss =  0.00018416203965898603\n",
            "i =  1280\n",
            "loss =  3.909069346264005e-05\n",
            "i =  1920\n",
            "loss =  2.2724829250364564e-05\n",
            "i =  2560\n",
            "loss =  6.540248705277918e-06\n",
            "i =  0\n",
            "loss =  7.836959412088618e-06\n",
            "i =  640\n",
            "loss =  6.588341784663498e-05\n",
            "i =  1280\n",
            "loss =  0.00833199918270111\n",
            "i =  1920\n",
            "loss =  0.00012099757441319525\n",
            "i =  2560\n",
            "loss =  0.0001322684984188527\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from thop import profile\n",
        "import time\n",
        "\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "# device = 'cpu'\n",
        "\n",
        "batch_size = 32\n",
        "lr = 2e-5\n",
        "EPOCHS = 18\n",
        "\n",
        "def shuffle_dataset(journal, title, abstruct, label):\n",
        "    length = len(journal)\n",
        "    rng = np.random.default_rng(12345)\n",
        "    index = np.arange(length)\n",
        "    # print(index)\n",
        "    rng.shuffle(index)\n",
        "    # print(index)\n",
        "    return journal[index], title[index], abstruct[index], label[index]\n",
        "\n",
        "def evaluate(model, test_journal, test_title, test_abstruct, test_Y):\n",
        "    model.eval()\n",
        "    pred = []\n",
        "    logits = []\n",
        "    true_Y = []\n",
        "    length = len(test_Y)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(test_Y), batch_size):\n",
        "        # for i in range(0, 10, batch_size):\n",
        "            logit = model(test_journal[i:min(len(test_Y),i+batch_size)], test_title[i:min(len(test_Y),i+batch_size)], test_abstruct[i:min(len(test_Y),i+batch_size)]).cpu()\n",
        "            logits.extend(logit)\n",
        "            y_pred = torch.argmax(logit, dim=1).cpu()\n",
        "            pred.extend(y_pred)\n",
        "            true_Y.extend(test_Y[i:min(len(test_Y),i+batch_size)].cpu())\n",
        "        # print(true_Y)\n",
        "        # print(logits)\n",
        "        # print(pred)\n",
        "        true_label = []\n",
        "        prob = []\n",
        "        pred_label = []\n",
        "        for i in range(len(true_Y)):\n",
        "          true_label.append(true_Y[i].item())\n",
        "          prob.append(logits[i][1].item())\n",
        "          pred_label.append(pred[i].item())\n",
        "        # print(true_label)\n",
        "        # print(prob)\n",
        "        # print(pred_label)\n",
        "\n",
        "        # 计算specificity、sensitivity\n",
        "        tn, fp, fn, tp = confusion_matrix(true_label, pred_label).ravel()\n",
        "        specificity = tn / (tn + fp)\n",
        "        sensitivity = tp / (tp + fn)\n",
        "\n",
        "    return {\n",
        "        'label':true_label, 'proba':prob,\n",
        "        'AUC':roc_auc_score(true_label, prob),\n",
        "        'classification_report':classification_report(true_label, pred_label, digits=4),\n",
        "        'specificity': specificity,\n",
        "        'sensitivity': sensitivity\n",
        "    }\n",
        "\n",
        "\n",
        "def main():\n",
        "    # 按0和1生成训练集和测试集\n",
        "    X_0 = pd.read_excel('/content/drive/MyDrive/pajo_data/X_0.xlsx').values\n",
        "    Y_0 = pd.read_excel('/content/drive/MyDrive/pajo_data/Y_0.xlsx').values\n",
        "    X_1 = pd.read_excel('/content/drive/MyDrive/pajo_data/X_1.xlsx').values\n",
        "    Y_1 = pd.read_excel('/content/drive/MyDrive/pajo_data/Y_1.xlsx').values\n",
        "    title_0 = np.load('/content/drive/MyDrive/pajo_data/token_title_0.npy')\n",
        "    title_1 = np.load('/content/drive/MyDrive/pajo_data/token_title_1.npy')\n",
        "    abstruct_0 = np.load('/content/drive/MyDrive/pajo_data/token_abstruct_0_512.npy')\n",
        "    abstruct_1 = np.load('/content/drive/MyDrive/pajo_data/token_abstruct_1_512.npy')\n",
        "\n",
        "    rs = np.random.RandomState(42)\n",
        "    L = list(rs.randint(0, len(X_0), int(7/3*len(X_1))))\n",
        "    X_0 = X_0[L]\n",
        "    Y_0 = Y_0[L]\n",
        "    title_0=title_0[L]\n",
        "    abstruct_0 = abstruct_0[L]\n",
        "\n",
        "    journal_train_X_0, journal_test_X_0, train_Y_0,test_Y_0 = train_test_split(X_0, Y_0, train_size=0.80, random_state=42)\n",
        "    journal_train_X_1, journal_test_X_1, train_Y_1, test_Y_1 = train_test_split(X_1, Y_1, train_size=0.80, random_state=42)\n",
        "    title_train_X_0, title_test_X_0, _,_ = train_test_split(title_0, Y_0, train_size=0.80, random_state=42)\n",
        "    title_train_X_1, title_test_X_1, _,_ = train_test_split(title_1, Y_1, train_size=0.80, random_state=42)\n",
        "    abstruct_train_X_0, abstruct_test_X_0, _,_ = train_test_split(abstruct_0, Y_0, train_size=0.80, random_state=42)\n",
        "    abstruct_train_X_1, abstruct_test_X_1, _,_ = train_test_split(abstruct_1, Y_1, train_size=0.80, random_state=42)\n",
        "\n",
        "    test_journal = torch.from_numpy(np.vstack((journal_test_X_1, journal_test_X_0))).float().to(device)\n",
        "    test_title = torch.from_numpy(np.vstack((title_test_X_1, title_test_X_0))).to(device)\n",
        "    test_abstruct = torch.from_numpy(np.vstack((abstruct_test_X_1, abstruct_test_X_0))).to(device)\n",
        "    test_Y = torch.from_numpy(np.vstack((test_Y_1, test_Y_0))).to(device)\n",
        "    test_journal, test_title, test_abstruct, test_Y = shuffle_dataset(test_journal, test_title, test_abstruct, test_Y)\n",
        "\n",
        "\n",
        "    model = MyModel(embed_dim=768, journal_size=test_journal.shape[1]).to(device)\n",
        "#     model.load_state_dict(torch.load(\"./res/new_model_2_0.75.pt\"))\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "    journal_train_X_0 = torch.from_numpy(journal_train_X_0).float().to(device)\n",
        "    title_train_X_0 = torch.from_numpy(title_train_X_0).to(device)\n",
        "    abstruct_train_X_0 = torch.from_numpy(abstruct_train_X_0).to(device)\n",
        "    train_Y_0 = torch.from_numpy(train_Y_0).to(device)\n",
        "\n",
        "    journal_train_X_1 = torch.from_numpy(journal_train_X_1).float().float().to(device)\n",
        "    title_train_X_1 = torch.from_numpy(title_train_X_1).to(device)\n",
        "    abstruct_train_X_1 = torch.from_numpy(abstruct_train_X_1).to(device)\n",
        "    train_Y_1 = torch.from_numpy(train_Y_1).to(device)\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        start_time = time.time()\n",
        "        journal_train_X_0, title_train_X_0, abstruct_train_X_0, train_Y_0 = shuffle_dataset(journal_train_X_0, title_train_X_0, abstruct_train_X_0, train_Y_0)\n",
        "        train_journal = torch.cat((journal_train_X_1, journal_train_X_0))\n",
        "        # print('journal_train_X_1.shape:',journal_train_X_1.shape)\n",
        "        train_title = torch.cat((title_train_X_1, title_train_X_0))\n",
        "        train_abstruct = torch.cat((abstruct_train_X_1, abstruct_train_X_0))\n",
        "        train_Y = torch.cat((train_Y_1, train_Y_0))\n",
        "        train_journal, train_title, train_abstruct, train_Y = shuffle_dataset(train_journal, train_title, train_abstruct, train_Y)\n",
        "\n",
        "        for i in range(0, len(train_Y), batch_size):\n",
        "        # for i in range(0, 10, batch_size):\n",
        "            model.zero_grad()\n",
        "            logits = model(train_journal[i:min(len(train_Y),i+batch_size)], train_title[i:min(len(train_Y),i+batch_size)], train_abstruct[i:min(len(train_Y),i+batch_size)])\n",
        "            loss = focal_loss(logits[:,1].unsqueeze(1), train_Y[i:i+batch_size])\n",
        "            loss.backward()\n",
        "            if((i/batch_size)%20 == 0):\n",
        "              print('i = ', i)\n",
        "              print('loss = ', loss.item())\n",
        "            optimizer.step()\n",
        "\n",
        "        end_time = time.time()\n",
        "\n",
        "        # 计算每秒钟的训练样本数（即MLOPS）\n",
        "        num_samples = len(train_Y)\n",
        "        training_time = end_time - start_time\n",
        "        samples_per_second = num_samples / training_time\n",
        "\n",
        "        if(epoch%3 == 0):\n",
        "\n",
        "              print('evaluating...')\n",
        "\n",
        "              val_metrics = evaluate(model, test_journal, test_title, test_abstruct, test_Y)\n",
        "\n",
        "              print('epochs:',epoch)\n",
        "              print('PAJO-T AUC = ',val_metrics['AUC'])\n",
        "              print('Specificity = ', val_metrics['specificity'])\n",
        "              print('Sensitivity = ', val_metrics['sensitivity'])\n",
        "              # 输出模型信息\n",
        "              flops, params = profile(model, inputs=(train_journal[0:batch_size], train_title[0:batch_size], train_abstruct[0:batch_size]))\n",
        "              print(\"Flops: {:.2f}\".format(flops))\n",
        "              print(\"MLOPS: {:.2f}\".format(samples_per_second))\n",
        "              print(val_metrics['classification_report'])\n",
        "\n",
        "              torch.save(model.state_dict(), './res/no_key_2_0.80_title_{}.pt'.format(epoch))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    if not os.path.exists('./res'):\n",
        "        os.mkdir('./res')\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3k7Vr45Kb7ly"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings(action = 'ignore')\n",
        "\n",
        "def best_yuzhi_aimed_at_1(preda,y_test):\n",
        "    #preda为预测为1类的概率,输入形式为narray\n",
        "    precisions = []\n",
        "    recalls = []\n",
        "    for i in np.arange(0.01,1,0.01):\n",
        "        y_pred = np.where(preda<i,0,1)\n",
        "        TN,FP,FN,TP = metrics.confusion_matrix(y_test,y_pred).ravel()\n",
        "        precision = TP/(TP+FP)\n",
        "        recall = TP/(TP+FN)\n",
        "        precisions.append(precision)\n",
        "        recalls.append(recall)\n",
        "    precisions = np.array(precisions)\n",
        "    recalls = np.array(recalls)\n",
        "    f1_scores = (2 * precisions * recalls) / (precisions + recalls)\n",
        "    best_f1_score = np.max(f1_scores[np.isfinite(f1_scores)])\n",
        "    best_f1_score_index = np.argmax(f1_scores[np.isfinite(f1_scores)])\n",
        "    return best_f1_score, np.arange(0.01,1,0.01)[best_f1_score_index]\n",
        "#调用示例\n",
        "y_test=np.array([0,0,1,1,1,0,1,0,1,0,1,1])\n",
        "preda = np.array([0.2,0.3,0.4,0.44,0.45,0.56,0.3,0.1,0.7,0.9,0.13,0.5])\n",
        "print(best_yuzhi_aimed_at_1(preda,y_test))\n",
        "#输出\n",
        "(0.7777777777777778, 0.11)#（1类最佳f1值和对应的阈值）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Mb-hnXab7ly"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "def plot_picture(y_test,probas):\n",
        "    # y_test测试集\n",
        "    # probas预测概率\n",
        "    CVD = pd.DataFrame()\n",
        "    CVD['正样本'] = y_test\n",
        "    CVD['score'] = probas\n",
        "    CVD = CVD.sort_values(by='score')\n",
        "    cvd_risk = CVD.reset_index(drop=True)\n",
        "    print(cvd_risk)\n",
        "    H = len(cvd_risk)\n",
        "    h = int(H / 10)\n",
        "    cvd = []\n",
        "    risk = []\n",
        "    h0 = 0\n",
        "    risk_count = 0\n",
        "    cvd_count = 0\n",
        "    for i in range(len(cvd_risk)):\n",
        "        if h0 + h > i + 1:\n",
        "            risk_count = risk_count + cvd_risk.loc[i, \"score\"]\n",
        "            if cvd_risk.loc[i, \"正样本\"] == 1:\n",
        "                cvd_count = cvd_count + 1\n",
        "        else:\n",
        "            h0 = h0 + h\n",
        "            cvd.append(round(cvd_count / h, 3))\n",
        "            risk.append(round(risk_count / h, 3))\n",
        "            risk_count = 0\n",
        "            cvd_count = 0\n",
        "\n",
        "    labels = ['10', '9', '8', '7', '6', '5', '4', '3', '2', '1']\n",
        "    cvd.reverse()\n",
        "\n",
        "    x = np.arange(len(labels))  # the label locations\n",
        "    width = 0.8  # the width of the bars\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    rects1 = ax.bar(x, cvd, width, color='royalblue')\n",
        "    # rects2 = ax.bar(x + width / 2, risk, width, label='Estimated', color='indianred')\n",
        "\n",
        "    # Add some text for labels, title and custom x-axis tick labels, etc.\n",
        "    ax.set_ylabel('Proportion of Positive Samples')\n",
        "    ax.set_xlabel('Decile of Estimated Score')\n",
        "    # ax.set_title('Observed Vs Estimated')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(labels)\n",
        "    ax.legend()\n",
        "    plt.axhline(y=np.mean(cvd),  linestyle='--', color='black')\n",
        "    def autolabel(rects):\n",
        "        \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
        "        for rect in rects:\n",
        "            height = rect.get_height()\n",
        "            ax.annotate('{}'.format(height),\n",
        "                        xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                        xytext=(0, 2),  # 3 points vertical offset\n",
        "                        textcoords=\"offset points\",\n",
        "                        ha='center', va='bottom')\n",
        "\n",
        "    autolabel(rects1)\n",
        "    # autolabel(rects2)\n",
        "    fig.tight_layout()\n",
        "    plt.savefig(\"柱状图.png\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUPXnB6Jb7ly"
      },
      "outputs": [],
      "source": [
        "def yuzhi(preda,door=0.1):\n",
        "    predict=[]\n",
        "    for i in range(len(preda)):\n",
        "        if preda[i] < door:\n",
        "            predict.append(0)\n",
        "        else:\n",
        "            predict.append(1)\n",
        "    return predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWLhZVCbb7ly"
      },
      "outputs": [],
      "source": [
        "#按阈值最大\n",
        "from sklearn.metrics import roc_auc_score,roc_curve,auc\n",
        "from numpy import argmax\n",
        "def find_optimal_cutoff(tpr,fpr,threshold):\n",
        "    optimal_idx = np.argmax(tpr - fpr)\n",
        "    optimal_threshold = threshold[optimal_idx]\n",
        "    return optimal_threshold\n",
        "\n",
        "def best_confusion_matrix(y_test, y_test_predprob):\n",
        "    \"\"\"\n",
        "        根据真实值和预测值（预测概率）的向量来计算混淆矩阵和最优的划分阈值\n",
        "\n",
        "        Args:\n",
        "            y_test:真实值\n",
        "            y_test_predprob：预测值\n",
        "\n",
        "        Returns:\n",
        "            返回最佳划分阈值和混淆矩阵\n",
        "        \"\"\"\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, y_test_predprob, pos_label=1)\n",
        "    cutoff = find_optimal_cutoff(tpr,fpr,thresholds)\n",
        "    y_pred = yuzhi(y_test_predprob,cutoff)\n",
        "    print(metrics.confusion_matrix(y_true=y_test, y_pred=y_pred))\n",
        "    print(metrics.classification_report(y_true=y_test, y_pred=y_pred))\n",
        "    TN,FP,FN,TP = metrics.confusion_matrix(y_test,y_pred).ravel()\n",
        "    return cutoff,TN,FN,FP,TP\n",
        "best_confusion_matrix(test_Y_input,preda)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgY8v3eDb7lz"
      },
      "outputs": [],
      "source": [
        "precisions, recalls, thresholds = precision_recall_curve(test_Y_input,predict)\n",
        "\n",
        "# 拿到最优结果以及索引\n",
        "f1_scores = (2 * precisions * recalls) / (precisions + recalls)\n",
        "best_f1_score = np.max(f1_scores[np.isfinite(f1_scores)])\n",
        "best_f1_score_index = np.argmax(f1_scores[np.isfinite(f1_scores)])\n",
        "\n",
        "# 阈值\n",
        "best_f1_score, thresholds[best_f1_score_index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-4S-nk4w61b",
        "outputId": "b72699f0-3c33-4c5e-df9a-a37e7b051c80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  data.zip\n",
            "   creating: data/\n",
            "  inflating: data/token_abstruct_0_400.npy  \n",
            "  inflating: data/token_abstruct_1_400.npy  \n",
            "  inflating: data/token_title_0.npy  \n",
            "  inflating: data/token_title_1.npy  \n",
            "  inflating: data/X_0.xlsx           \n",
            "  inflating: data/X_1.xlsx           \n",
            "  inflating: data/Y_0.xlsx           \n",
            "  inflating: data/Y_1.xlsx           \n",
            "   creating: data/__pycache__/\n",
            "  inflating: data/__pycache__/dataset.cpython-37.pyc  \n"
          ]
        }
      ],
      "source": [
        "!unzip data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qw7zyYoiZiaI",
        "outputId": "49f393f2-2f40-4461-ffd7-5a869e7a6b79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch_metric_learning\n",
            "  Downloading pytorch_metric_learning-1.5.2-py3-none-any.whl (111 kB)\n",
            "\u001b[K     |████████████████████████████████| 111 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_metric_learning) (4.64.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_metric_learning) (1.21.6)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from pytorch_metric_learning) (0.13.1+cu113)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_metric_learning) (1.12.1+cu113)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pytorch_metric_learning) (1.0.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (4.1.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pytorch_metric_learning) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pytorch_metric_learning) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pytorch_metric_learning) (1.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->pytorch_metric_learning) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->pytorch_metric_learning) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->pytorch_metric_learning) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->pytorch_metric_learning) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->pytorch_metric_learning) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->pytorch_metric_learning) (3.0.4)\n",
            "Installing collected packages: pytorch-metric-learning\n",
            "Successfully installed pytorch-metric-learning-1.5.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch_metric_learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0nsOF6V8Vf3",
        "outputId": "e93848d5-fa12-4bd9-8d8a-32241c5fe381"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.21.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqYGm62_8a4-",
        "outputId": "d01f4b10-31bb-4d30-f002-b7c1c59424c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.21.1-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 5.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 47.9 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 47.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 12.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.21.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZ4E0YDZZXh1",
        "outputId": "257db48b-6628-40a4-d2fd-dc09b3cf881c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  data.zip\n",
            "   creating: data/\n",
            "  inflating: data/token_abstruct_0_400.npy  \n",
            "  inflating: data/token_abstruct_1_400.npy  \n",
            "  inflating: data/token_title_0.npy  \n",
            "  inflating: data/token_title_1.npy  \n",
            "  inflating: data/X_0.xlsx           \n",
            "  inflating: data/X_1.xlsx           \n",
            "  inflating: data/Y_0.xlsx           \n",
            "  inflating: data/Y_1.xlsx           \n",
            "   creating: data/__pycache__/\n",
            "  inflating: data/__pycache__/dataset.cpython-37.pyc  \n"
          ]
        }
      ],
      "source": [
        "!unzip data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUwm114yjvuD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}