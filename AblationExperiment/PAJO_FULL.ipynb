{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9H6Gu5EKHVb5",
        "outputId": "a3c642c1-336f-484e-873e-780c79be3239"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1lakAphH7Ir",
        "outputId": "e5d34a9b-812d-4bc9-8ca5-9afd0c4dad47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Collecting pytorch\n",
            "  Using cached pytorch-1.0.2.tar.gz (689 bytes)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pytorch\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for pytorch (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for pytorch\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for pytorch\n",
            "Failed to build pytorch\n",
            "\u001b[31mERROR: Could not build wheels for pytorch, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: sklearn in /usr/local/lib/python3.10/dist-packages (0.0.post5)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install pytorch\n",
        "!pip install sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Auk8ljxTwQCJ"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoModel\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self, embed_dim, journal_size):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.bert = AutoModel.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\")\n",
        "\n",
        "        # self.tokenizer = AutoTokenizer.from_pretrained(\"scibert_scivocab_uncased\")\n",
        "        self.atten = nn.MultiheadAttention(embed_dim=embed_dim,num_heads=8,dropout=0.1)\n",
        "        self.liner_query = nn.Linear(embed_dim, embed_dim)\n",
        "        self.liner_key = nn.Linear(embed_dim, embed_dim)\n",
        "        self.liner_value = nn.Linear(embed_dim, embed_dim)\n",
        "        self.liner1 = nn.Linear(journal_size, journal_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.liner2 = nn.Linear(journal_size + embed_dim*2, 2)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def get_sentence_feature(self, input_ids):\n",
        "        outputs = self.bert(input_ids)\n",
        "        pooled_output = outputs[1]\n",
        "        # print('pooled_output.shape:',pooled_output.shape)\n",
        "        return pooled_output\n",
        "\n",
        "    def forward(self, journal, title, abasruct):\n",
        "#         print('journal.shape',journal.shape)\n",
        "#         print('title.shape',title.shape)\n",
        "#         print('abasruct.shape',abasruct.shape)\n",
        "        title_vector = self.get_sentence_feature(title).unsqueeze(0)\n",
        "        abasruct_vector = self.get_sentence_feature(abasruct).unsqueeze(0)\n",
        "#         print('title.shape',title_vector.shape)\n",
        "#         print('abasruct.shape',abasruct_vector.shape)\n",
        "        title_query_vector, title_key_vector, title_value_vector = self.liner_query(title_vector),self.liner_key(title_vector),self.liner_value(title_vector)\n",
        "        title_atten,_ = self.atten(title_query_vector, title_key_vector, title_value_vector)\n",
        "        abasruct_query_vector, abasruct_key_vector, abasruct_value_vector = self.liner_query(abasruct_vector),self.liner_key(abasruct_vector),self.liner_value(abasruct_vector)\n",
        "        abasruct_atten,_ = self.atten(abasruct_query_vector, abasruct_key_vector, abasruct_value_vector)\n",
        "        journal_vector = self.liner1(journal)\n",
        "        journal_vector = self.relu(journal_vector)\n",
        "        # print('journal_vector.shape',journal_vector.shape)\n",
        "#         print('title_atten.shape',title_atten.shape)\n",
        "#         print('abasruct_atten.shape',abasruct_atten.shape)\n",
        "        feature = torch.cat((journal_vector, title_atten.squeeze(0), abasruct_atten.squeeze(0)), 1)\n",
        "        # feature = torch.cat((journal_vector, title_vector, abasruct_vector), 1)\n",
        "        out = self.liner2(feature)\n",
        "        output = self.softmax(out)\n",
        "        return output\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rx3JgKEvF6X1"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "def focal_loss(\n",
        "    inputs: torch.Tensor,\n",
        "    targets: torch.Tensor,\n",
        "    alpha: float = 0.80,#0.40\n",
        "    gamma: float = 2,\n",
        "    reduction: str = \"mean\",\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        inputs: A float tensor of arbitrary shape.\n",
        "                The predictions which have been sigmod for each example.\n",
        "        targets: A float tensor with the same shape as inputs. Stores the binary\n",
        "                 classification label for each element in inputs\n",
        "                (0 for the negative class and 1 for the positive class).\n",
        "        alpha: (optional) Weighting factor in range (0,1) to balance\n",
        "                positive vs negative examples. Default = -1 (no weighting).\n",
        "        gamma: Exponent of the modulating factor (1 - p_t) to\n",
        "               balance easy vs hard examples.\n",
        "        reduction: 'none' | 'mean' | 'sum'\n",
        "                 'none': No reduction will be applied to the output.\n",
        "                 'mean': The output will be averaged.\n",
        "                 'sum': The output will be summed.\n",
        "    Returns:\n",
        "        Loss tensor with the reduction option applied.\n",
        "    \"\"\"\n",
        "    inputs = inputs.float()\n",
        "    targets = targets.float()\n",
        "    p = inputs\n",
        "    ce_loss = F.binary_cross_entropy(inputs, targets, reduction=\"none\")\n",
        "    p_t = p * targets + (1 - p) * (1 - targets)\n",
        "    loss = ce_loss * ((1 - p_t) ** gamma)\n",
        "\n",
        "    if alpha >= 0:\n",
        "        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n",
        "        loss = alpha_t * loss\n",
        "\n",
        "    if reduction == \"mean\":\n",
        "        loss = loss.mean()\n",
        "    elif reduction == \"sum\":\n",
        "        loss = loss.sum()\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOhpZGKRgx5o",
        "outputId": "7c651779-575f-4f57-b7ff-4dc1d66a7fc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: thop in /usr/local/lib/python3.10/dist-packages (0.1.1.post2209072238)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from thop) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->thop) (4.6.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->thop) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->thop) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->thop) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->thop) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->thop) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install thop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHqo2jKkwPy6",
        "outputId": "42f1c036-cd99-46ff-ffb5-2a015f36e3dc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "i =  0\n",
            "loss =  0.08659809827804565\n",
            "i =  320\n",
            "loss =  0.06346247345209122\n",
            "i =  640\n",
            "loss =  0.0602252222597599\n",
            "i =  960\n",
            "loss =  0.06306563317775726\n",
            "i =  1280\n",
            "loss =  0.04753667116165161\n",
            "i =  1600\n",
            "loss =  0.04531744867563248\n",
            "i =  1920\n",
            "loss =  0.03300156816840172\n",
            "i =  2240\n",
            "loss =  0.04455246776342392\n",
            "i =  2560\n",
            "loss =  0.033176690340042114\n",
            "evaluating...\n",
            "epochs: 0\n",
            "PAJO-full AUC =  0.8970817554020941\n",
            "Specificity =  0.7611940298507462\n",
            "Sensitivity =  0.8606965174129353\n",
            "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
            "[INFO] Register count_softmax() for <class 'torch.nn.modules.activation.Softmax'>.\n",
            "Flops: 761778197072.00\n",
            "MLOPS: 9.72\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9273    0.7612    0.8361       469\n",
            "           1     0.6070    0.8607    0.7119       201\n",
            "\n",
            "    accuracy                         0.7910       670\n",
            "   macro avg     0.7671    0.8109    0.7740       670\n",
            "weighted avg     0.8312    0.7910    0.7988       670\n",
            "\n",
            "i =  0\n",
            "loss =  0.025893643498420715\n",
            "i =  320\n",
            "loss =  0.03792249783873558\n",
            "i =  640\n",
            "loss =  0.033226378262043\n",
            "i =  960\n",
            "loss =  0.05181103199720383\n",
            "i =  1280\n",
            "loss =  0.030360201373696327\n",
            "i =  1600\n",
            "loss =  0.02969355136156082\n",
            "i =  1920\n",
            "loss =  0.024211693555116653\n",
            "i =  2240\n",
            "loss =  0.03748795762658119\n",
            "i =  2560\n",
            "loss =  0.025722835212945938\n",
            "i =  0\n",
            "loss =  0.019216423854231834\n",
            "i =  320\n",
            "loss =  0.024255987256765366\n",
            "i =  640\n",
            "loss =  0.05427384376525879\n",
            "i =  960\n",
            "loss =  0.06320196390151978\n",
            "i =  1280\n",
            "loss =  0.05264993757009506\n",
            "i =  1600\n",
            "loss =  0.062264446169137955\n",
            "i =  1920\n",
            "loss =  0.0673498660326004\n",
            "i =  2240\n",
            "loss =  0.05354144424200058\n",
            "i =  2560\n",
            "loss =  0.07535722851753235\n",
            "i =  0\n",
            "loss =  0.0648086816072464\n",
            "i =  320\n",
            "loss =  0.05129622668027878\n",
            "i =  640\n",
            "loss =  0.06643117964267731\n",
            "i =  960\n",
            "loss =  0.05169728398323059\n",
            "i =  1280\n",
            "loss =  0.029154155403375626\n",
            "i =  1600\n",
            "loss =  0.015096256509423256\n",
            "i =  1920\n",
            "loss =  0.015067843720316887\n",
            "i =  2240\n",
            "loss =  0.026846488937735558\n",
            "i =  2560\n",
            "loss =  0.01698426343500614\n",
            "evaluating...\n",
            "epochs: 3\n",
            "PAJO-full AUC =  0.913524064114396\n",
            "Specificity =  0.8166311300639659\n",
            "Sensitivity =  0.8557213930348259\n",
            "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
            "[INFO] Register count_softmax() for <class 'torch.nn.modules.activation.Softmax'>.\n",
            "Flops: 761778197072.00\n",
            "MLOPS: 9.74\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9296    0.8166    0.8695       469\n",
            "           1     0.6667    0.8557    0.7495       201\n",
            "\n",
            "    accuracy                         0.8284       670\n",
            "   macro avg     0.7981    0.8362    0.8095       670\n",
            "weighted avg     0.8507    0.8284    0.8335       670\n",
            "\n",
            "i =  0\n",
            "loss =  0.014619078487157822\n",
            "i =  320\n",
            "loss =  0.0185244083404541\n",
            "i =  640\n",
            "loss =  0.029952436685562134\n",
            "i =  960\n",
            "loss =  0.0112405214458704\n",
            "i =  1280\n",
            "loss =  0.03182356432080269\n",
            "i =  1600\n",
            "loss =  0.006898403633385897\n",
            "i =  1920\n",
            "loss =  0.01048310101032257\n",
            "i =  2240\n",
            "loss =  0.022593382745981216\n",
            "i =  2560\n",
            "loss =  0.008920526131987572\n",
            "i =  0\n",
            "loss =  0.01960434764623642\n",
            "i =  320\n",
            "loss =  0.037070851773023605\n",
            "i =  640\n",
            "loss =  0.028234148398041725\n",
            "i =  960\n",
            "loss =  0.013070652261376381\n",
            "i =  1280\n",
            "loss =  0.011912564747035503\n",
            "i =  1600\n",
            "loss =  0.016527708619832993\n",
            "i =  1920\n",
            "loss =  0.008565215393900871\n",
            "i =  2240\n",
            "loss =  0.009696641936898232\n",
            "i =  2560\n",
            "loss =  0.020336005836725235\n",
            "i =  0\n",
            "loss =  0.01303916610777378\n",
            "i =  320\n",
            "loss =  0.004598655737936497\n",
            "i =  640\n",
            "loss =  0.01358705572783947\n",
            "i =  960\n",
            "loss =  0.03002397157251835\n",
            "i =  1280\n",
            "loss =  0.004676926881074905\n",
            "i =  1600\n",
            "loss =  0.017204346135258675\n",
            "i =  1920\n",
            "loss =  0.010261744260787964\n",
            "i =  2240\n",
            "loss =  0.015435786917805672\n",
            "i =  2560\n",
            "loss =  0.010214623995125294\n",
            "evaluating...\n",
            "epochs: 6\n",
            "PAJO-full AUC =  0.9107766073682758\n",
            "Specificity =  0.8656716417910447\n",
            "Sensitivity =  0.8109452736318408\n",
            "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
            "[INFO] Register count_softmax() for <class 'torch.nn.modules.activation.Softmax'>.\n",
            "Flops: 761778197072.00\n",
            "MLOPS: 9.71\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9144    0.8657    0.8894       469\n",
            "           1     0.7212    0.8109    0.7635       201\n",
            "\n",
            "    accuracy                         0.8493       670\n",
            "   macro avg     0.8178    0.8383    0.8264       670\n",
            "weighted avg     0.8565    0.8493    0.8516       670\n",
            "\n",
            "i =  0\n",
            "loss =  0.005320187658071518\n",
            "i =  320\n",
            "loss =  0.00039048618054948747\n",
            "i =  640\n",
            "loss =  0.028340281918644905\n",
            "i =  960\n",
            "loss =  0.015619978308677673\n",
            "i =  1280\n",
            "loss =  0.002574827754870057\n",
            "i =  1600\n",
            "loss =  0.0023876680061221123\n",
            "i =  1920\n",
            "loss =  0.0082100760191679\n",
            "i =  2240\n",
            "loss =  0.018342874944210052\n",
            "i =  2560\n",
            "loss =  0.0009788789320737123\n",
            "i =  0\n",
            "loss =  0.005633385851979256\n",
            "i =  320\n",
            "loss =  0.004410168156027794\n",
            "i =  640\n",
            "loss =  0.0005042169359512627\n",
            "i =  960\n",
            "loss =  0.0040585435926914215\n",
            "i =  1280\n",
            "loss =  0.002298885490745306\n",
            "i =  1600\n",
            "loss =  0.0022049422841519117\n",
            "i =  1920\n",
            "loss =  0.027581047266721725\n",
            "i =  2240\n",
            "loss =  0.005108961835503578\n",
            "i =  2560\n",
            "loss =  0.005641943775117397\n",
            "i =  0\n",
            "loss =  0.0026348207611590624\n",
            "i =  320\n",
            "loss =  0.010471383109688759\n",
            "i =  640\n",
            "loss =  0.01037575677037239\n",
            "i =  960\n",
            "loss =  0.0019203107804059982\n",
            "i =  1280\n",
            "loss =  0.002621568739414215\n",
            "i =  1600\n",
            "loss =  0.0036239575129002333\n",
            "i =  1920\n",
            "loss =  4.750301013700664e-05\n",
            "i =  2240\n",
            "loss =  0.02437279373407364\n",
            "i =  2560\n",
            "loss =  0.0012509548105299473\n",
            "evaluating...\n",
            "epochs: 9\n",
            "PAJO-full AUC =  0.9046664332919623\n",
            "Specificity =  0.8507462686567164\n",
            "Sensitivity =  0.8159203980099502\n",
            "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
            "[INFO] Register count_softmax() for <class 'torch.nn.modules.activation.Softmax'>.\n",
            "Flops: 761778197072.00\n",
            "MLOPS: 9.74\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9151    0.8507    0.8818       469\n",
            "           1     0.7009    0.8159    0.7540       201\n",
            "\n",
            "    accuracy                         0.8403       670\n",
            "   macro avg     0.8080    0.8333    0.8179       670\n",
            "weighted avg     0.8509    0.8403    0.8434       670\n",
            "\n",
            "i =  0\n",
            "loss =  0.006778612267225981\n",
            "i =  320\n",
            "loss =  0.00032566554727964103\n",
            "i =  640\n",
            "loss =  0.001960900379344821\n",
            "i =  960\n",
            "loss =  0.0001054789827321656\n",
            "i =  1280\n",
            "loss =  6.81028250255622e-05\n",
            "i =  1600\n",
            "loss =  0.0013832877157256007\n",
            "i =  1920\n",
            "loss =  0.000476884248200804\n",
            "i =  2240\n",
            "loss =  0.00018388728494755924\n",
            "i =  2560\n",
            "loss =  0.0038633670192211866\n",
            "i =  0\n",
            "loss =  0.0006541743641719222\n",
            "i =  320\n",
            "loss =  0.056761302053928375\n",
            "i =  640\n",
            "loss =  0.013287914916872978\n",
            "i =  960\n",
            "loss =  0.0038048552814871073\n",
            "i =  1280\n",
            "loss =  0.0004374647978693247\n",
            "i =  1600\n",
            "loss =  0.009661434218287468\n",
            "i =  1920\n",
            "loss =  0.020574014633893967\n",
            "i =  2240\n",
            "loss =  0.007985605858266354\n",
            "i =  2560\n",
            "loss =  0.0023265338968485594\n",
            "i =  0\n",
            "loss =  0.0004620424588210881\n",
            "i =  320\n",
            "loss =  0.00030962959863245487\n",
            "i =  640\n",
            "loss =  0.00018757009820546955\n",
            "i =  960\n",
            "loss =  0.0007016730960458517\n",
            "i =  1280\n",
            "loss =  0.0004135607450734824\n",
            "i =  1600\n",
            "loss =  0.0006518143345601857\n",
            "i =  1920\n",
            "loss =  0.00011372046719770879\n",
            "i =  2240\n",
            "loss =  0.0012502315221354365\n",
            "i =  2560\n",
            "loss =  0.00015887035988271236\n",
            "evaluating...\n",
            "epochs: 12\n",
            "PAJO-full AUC =  0.8927643233724767\n",
            "Specificity =  0.9104477611940298\n",
            "Sensitivity =  0.7064676616915423\n",
            "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
            "[INFO] Register count_softmax() for <class 'torch.nn.modules.activation.Softmax'>.\n",
            "Flops: 761778197072.00\n",
            "MLOPS: 9.74\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8786    0.9104    0.8942       469\n",
            "           1     0.7717    0.7065    0.7377       201\n",
            "\n",
            "    accuracy                         0.8493       670\n",
            "   macro avg     0.8252    0.8085    0.8160       670\n",
            "weighted avg     0.8465    0.8493    0.8473       670\n",
            "\n",
            "i =  0\n",
            "loss =  0.00037265746504999697\n",
            "i =  320\n",
            "loss =  0.0006510459934361279\n",
            "i =  640\n",
            "loss =  0.000901116814929992\n",
            "i =  960\n",
            "loss =  5.986411269987002e-05\n",
            "i =  1280\n",
            "loss =  0.00237430352717638\n",
            "i =  1600\n",
            "loss =  0.025970106944441795\n",
            "i =  1920\n",
            "loss =  0.00010475035378476605\n",
            "i =  2240\n",
            "loss =  0.00031356068211607635\n",
            "i =  2560\n",
            "loss =  0.002020298969000578\n",
            "i =  0\n",
            "loss =  0.0002607563219498843\n",
            "i =  320\n",
            "loss =  7.625769285368733e-06\n",
            "i =  640\n",
            "loss =  0.0002722577773965895\n",
            "i =  960\n",
            "loss =  0.0010648454772308469\n",
            "i =  1280\n",
            "loss =  8.147577545969398e-07\n",
            "i =  1600\n",
            "loss =  7.699323759879917e-05\n",
            "i =  1920\n",
            "loss =  1.5139728020585608e-05\n",
            "i =  2240\n",
            "loss =  0.00011629883374553174\n",
            "i =  2560\n",
            "loss =  0.00013684947043657303\n",
            "i =  0\n",
            "loss =  0.000586680369451642\n",
            "i =  320\n",
            "loss =  0.00020015425980091095\n",
            "i =  640\n",
            "loss =  3.4386484912829474e-05\n",
            "i =  960\n",
            "loss =  0.0010234579676762223\n",
            "i =  1280\n",
            "loss =  4.766217534779571e-05\n",
            "i =  1600\n",
            "loss =  0.0020189739298075438\n",
            "i =  1920\n",
            "loss =  0.00010608002776280046\n",
            "i =  2240\n",
            "loss =  0.00011046798317693174\n",
            "i =  2560\n",
            "loss =  4.494381937547587e-05\n",
            "evaluating...\n",
            "epochs: 15\n",
            "PAJO-full AUC =  0.9079973267988417\n",
            "Specificity =  0.906183368869936\n",
            "Sensitivity =  0.7512437810945274\n",
            "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
            "[INFO] Register count_softmax() for <class 'torch.nn.modules.activation.Softmax'>.\n",
            "Flops: 761778197072.00\n",
            "MLOPS: 9.73\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8947    0.9062    0.9004       469\n",
            "           1     0.7744    0.7512    0.7626       201\n",
            "\n",
            "    accuracy                         0.8597       670\n",
            "   macro avg     0.8345    0.8287    0.8315       670\n",
            "weighted avg     0.8586    0.8597    0.8591       670\n",
            "\n",
            "i =  0\n",
            "loss =  0.005517560988664627\n",
            "i =  320\n",
            "loss =  0.018246326595544815\n",
            "i =  640\n",
            "loss =  0.0004043617518618703\n",
            "i =  960\n",
            "loss =  0.0048561422154307365\n",
            "i =  1280\n",
            "loss =  8.540588896721601e-05\n",
            "i =  1600\n",
            "loss =  0.0006111579714342952\n",
            "i =  1920\n",
            "loss =  0.00026722587062977254\n",
            "i =  2240\n",
            "loss =  0.002573711797595024\n",
            "i =  2560\n",
            "loss =  0.002453510882332921\n",
            "i =  0\n",
            "loss =  4.817287845071405e-05\n",
            "i =  320\n",
            "loss =  1.1747408279916272e-05\n",
            "i =  640\n",
            "loss =  4.11039509344846e-05\n",
            "i =  960\n",
            "loss =  0.003273934591561556\n",
            "i =  1280\n",
            "loss =  4.805378011951689e-06\n",
            "i =  1600\n",
            "loss =  0.0005986566538922489\n",
            "i =  1920\n",
            "loss =  3.244702020310797e-05\n",
            "i =  2240\n",
            "loss =  0.00016180066450033337\n",
            "i =  2560\n",
            "loss =  1.6591295207035728e-05\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from thop import profile\n",
        "import time\n",
        "\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "#device = 'cpu'\n",
        "\n",
        "batch_size = 16\n",
        "lr = 2e-5\n",
        "EPOCHS = 18\n",
        "\n",
        "def shuffle_dataset(journal, title, abstruct, label):\n",
        "    length = len(journal)\n",
        "    rng = np.random.default_rng(12345)\n",
        "    index = np.arange(length)\n",
        "    # print(index)\n",
        "    rng.shuffle(index)\n",
        "    # print(index)\n",
        "    return journal[index], title[index], abstruct[index], label[index]\n",
        "\n",
        "def evaluate(model, test_journal, test_title, test_abstruct, test_Y):\n",
        "    model.eval()\n",
        "    pred = []\n",
        "    logits = []\n",
        "    true_Y = []\n",
        "    length = len(test_Y)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(test_Y), batch_size):\n",
        "        # for i in range(0, 10, batch_size):\n",
        "            logit = model(test_journal[i:min(len(test_Y),i+batch_size)], test_title[i:min(len(test_Y),i+batch_size)], test_abstruct[i:min(len(test_Y),i+batch_size)]).cpu()\n",
        "            logits.extend(logit)\n",
        "            y_pred = torch.argmax(logit, dim=1).cpu()\n",
        "            pred.extend(y_pred)\n",
        "            true_Y.extend(test_Y[i:min(len(test_Y),i+batch_size)].cpu())\n",
        "        # print(true_Y)\n",
        "        # print(logits)\n",
        "        # print(pred)\n",
        "        true_label = []\n",
        "        prob = []\n",
        "        pred_label = []\n",
        "        for i in range(len(true_Y)):\n",
        "          true_label.append(true_Y[i].item())\n",
        "          prob.append(logits[i][1].item())\n",
        "          pred_label.append(pred[i].item())\n",
        "        # print(true_label)\n",
        "        # print(prob)\n",
        "        # print(pred_label)\n",
        "\n",
        "        # 计算specificity、sensitivity\n",
        "        tn, fp, fn, tp = confusion_matrix(true_label, pred_label).ravel()\n",
        "        specificity = tn / (tn + fp)\n",
        "        sensitivity = tp / (tp + fn)\n",
        "\n",
        "    return {\n",
        "        'label':true_label, 'proba':prob,\n",
        "        'AUC':roc_auc_score(true_label, prob),\n",
        "        'classification_report':classification_report(true_label, pred_label, digits=4),\n",
        "        'specificity': specificity,\n",
        "        'sensitivity': sensitivity\n",
        "    }\n",
        "\n",
        "\n",
        "def main():\n",
        "    # 按0和1生成训练集和测试集\n",
        "    X_0 = pd.read_excel('/content/drive/MyDrive/pajo_data/X_0.xlsx').values\n",
        "    Y_0 = pd.read_excel('/content/drive/MyDrive/pajo_data/Y_0.xlsx').values\n",
        "    X_1 = pd.read_excel('/content/drive/MyDrive/pajo_data/X_1.xlsx').values\n",
        "    Y_1 = pd.read_excel('/content/drive/MyDrive/pajo_data/Y_1.xlsx').values\n",
        "    title_0 = np.load('/content/drive/MyDrive/pajo_data/token_title_0.npy')\n",
        "    title_1 = np.load('/content/drive/MyDrive/pajo_data/token_title_1.npy')\n",
        "    abstruct_0 = np.load('/content/drive/MyDrive/pajo_data/token_abstruct_0_512.npy')\n",
        "    abstruct_1 = np.load('/content/drive/MyDrive/pajo_data/token_abstruct_1_512.npy')\n",
        "\n",
        "    rs = np.random.RandomState(42)\n",
        "    L = list(rs.randint(0, len(X_0), int(7/3*len(X_1))))\n",
        "    X_0 = X_0[L]\n",
        "    Y_0 = Y_0[L]\n",
        "    title_0=title_0[L]\n",
        "    abstruct_0 = abstruct_0[L]\n",
        "\n",
        "    journal_train_X_0, journal_test_X_0, train_Y_0,test_Y_0 = train_test_split(X_0, Y_0, train_size=0.80, random_state=42)\n",
        "    journal_train_X_1, journal_test_X_1, train_Y_1, test_Y_1 = train_test_split(X_1, Y_1, train_size=0.80, random_state=42)\n",
        "    title_train_X_0, title_test_X_0, _,_ = train_test_split(title_0, Y_0, train_size=0.80, random_state=42)\n",
        "    title_train_X_1, title_test_X_1, _,_ = train_test_split(title_1, Y_1, train_size=0.80, random_state=42)\n",
        "    abstruct_train_X_0, abstruct_test_X_0, _,_ = train_test_split(abstruct_0, Y_0, train_size=0.80, random_state=42)\n",
        "    abstruct_train_X_1, abstruct_test_X_1, _,_ = train_test_split(abstruct_1, Y_1, train_size=0.80, random_state=42)\n",
        "\n",
        "    test_journal = torch.from_numpy(np.vstack((journal_test_X_1, journal_test_X_0))).float().to(device)\n",
        "    test_title = torch.from_numpy(np.vstack((title_test_X_1, title_test_X_0))).to(device)\n",
        "    test_abstruct = torch.from_numpy(np.vstack((abstruct_test_X_1, abstruct_test_X_0))).to(device)\n",
        "    test_Y = torch.from_numpy(np.vstack((test_Y_1, test_Y_0))).to(device)\n",
        "    test_journal, test_title, test_abstruct, test_Y = shuffle_dataset(test_journal, test_title, test_abstruct, test_Y)\n",
        "\n",
        "\n",
        "    model = MyModel(embed_dim=768, journal_size=test_journal.shape[1]).to(device)\n",
        "#     model.load_state_dict(torch.load(\"./res/new_model_2_0.75.pt\"))\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "    journal_train_X_0 = torch.from_numpy(journal_train_X_0).float().to(device)\n",
        "    title_train_X_0 = torch.from_numpy(title_train_X_0).to(device)\n",
        "    abstruct_train_X_0 = torch.from_numpy(abstruct_train_X_0).to(device)\n",
        "    train_Y_0 = torch.from_numpy(train_Y_0).to(device)\n",
        "\n",
        "    journal_train_X_1 = torch.from_numpy(journal_train_X_1).float().to(device)\n",
        "    title_train_X_1 = torch.from_numpy(title_train_X_1).to(device)\n",
        "    abstruct_train_X_1 = torch.from_numpy(abstruct_train_X_1).to(device)\n",
        "    train_Y_1 = torch.from_numpy(train_Y_1).to(device)\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        start_time = time.time()\n",
        "        journal_train_X_0, title_train_X_0, abstruct_train_X_0, train_Y_0 = shuffle_dataset(journal_train_X_0, title_train_X_0, abstruct_train_X_0, train_Y_0)\n",
        "        train_journal = torch.cat((journal_train_X_1, journal_train_X_0))\n",
        "        # print('journal_train_X_1.shape:',journal_train_X_1.shape)\n",
        "        train_title = torch.cat((title_train_X_1, title_train_X_0))\n",
        "        train_abstruct = torch.cat((abstruct_train_X_1, abstruct_train_X_0))\n",
        "        train_Y = torch.cat((train_Y_1, train_Y_0))\n",
        "        train_journal, train_title, train_abstruct, train_Y = shuffle_dataset(train_journal, train_title, train_abstruct, train_Y)\n",
        "\n",
        "        for i in range(0, len(train_Y), batch_size):\n",
        "        #for i in range(0, 10, batch_size):\n",
        "            model.zero_grad()\n",
        "            logits = model(train_journal[i:min(len(train_Y),i+batch_size)], train_title[i:min(len(train_Y),i+batch_size)], train_abstruct[i:min(len(train_Y),i+batch_size)])\n",
        "            loss = focal_loss(logits[:,1].unsqueeze(1), train_Y[i:i+batch_size])\n",
        "            loss.backward()\n",
        "            if((i/batch_size)%20 == 0):\n",
        "              print('i = ', i)\n",
        "              print('loss = ', loss.item())\n",
        "            optimizer.step()\n",
        "\n",
        "        end_time = time.time()\n",
        "\n",
        "        # 计算每秒钟的训练样本数（即MLOPS）\n",
        "        num_samples = len(train_Y)\n",
        "        training_time = end_time - start_time\n",
        "        samples_per_second = num_samples / training_time\n",
        "\n",
        "        if(epoch%3 == 0):\n",
        "\n",
        "              print('evaluating...')\n",
        "\n",
        "              val_metrics = evaluate(model, test_journal, test_title, test_abstruct, test_Y)\n",
        "\n",
        "              print('epochs:',epoch)\n",
        "              print('PAJO-full AUC = ',val_metrics['AUC'])\n",
        "              print('Specificity = ', val_metrics['specificity'])\n",
        "              print('Sensitivity = ', val_metrics['sensitivity'])\n",
        "              # 输出模型信息\n",
        "              flops, params = profile(model, inputs=(train_journal[0:batch_size], train_title[0:batch_size], train_abstruct[0:batch_size]))\n",
        "              print(\"Flops: {:.2f}\".format(flops))\n",
        "              print(\"MLOPS: {:.2f}\".format(samples_per_second))\n",
        "              print(val_metrics['classification_report'])\n",
        "\n",
        "              torch.save(model.state_dict(), './res/no_key_2_0.80_{}.pt'.format(epoch))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    if not os.path.exists('./res'):\n",
        "        os.mkdir('./res')\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fa25Fjh3RHwM"
      },
      "outputs": [],
      "source": [
        "!nvcc -V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CLQNlJyRMMk",
        "outputId": "a1c564c6-93df-4115-8b77-7000c7eb4faa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.0.1+cu118\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__)  #注意是双下划线"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZWnARFxMp1j",
        "outputId": "fca6357c-d12c-454d-897d-3b0f34b70504"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  data.zip\n",
            "warning [data.zip]:  494025 extra bytes at beginning or within zipfile\n",
            "  (attempting to process anyway)\n",
            "file #1:  bad zipfile offset (local header sig):  494025\n",
            "  (attempting to re-compensate)\n",
            "   creating: data/\n",
            "error: invalid zip file with overlapped components (possible zip bomb)\n"
          ]
        }
      ],
      "source": [
        "!unzip data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KiD2RqnBHMk1"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings(action = 'ignore')\n",
        "\n",
        "def best_yuzhi_aimed_at_1(preda,y_test):\n",
        "    #preda为预测为1类的概率,输入形式为narray\n",
        "    precisions = []\n",
        "    recalls = []\n",
        "    for i in np.arange(0.01,1,0.01):\n",
        "        y_pred = np.where(preda<i,0,1)\n",
        "        TN,FP,FN,TP = metrics.confusion_matrix(y_test,y_pred).ravel()\n",
        "        precision = TP/(TP+FP)\n",
        "        recall = TP/(TP+FN)\n",
        "        precisions.append(precision)\n",
        "        recalls.append(recall)\n",
        "    precisions = np.array(precisions)\n",
        "    recalls = np.array(recalls)\n",
        "    f1_scores = (2 * precisions * recalls) / (precisions + recalls)\n",
        "    best_f1_score = np.max(f1_scores[np.isfinite(f1_scores)])\n",
        "    best_f1_score_index = np.argmax(f1_scores[np.isfinite(f1_scores)])\n",
        "    return best_f1_score, np.arange(0.01,1,0.01)[best_f1_score_index]\n",
        "#调用示例\n",
        "y_test=np.array([0,0,1,1,1,0,1,0,1,0,1,1])\n",
        "preda = np.array([0.2,0.3,0.4,0.44,0.45,0.56,0.3,0.1,0.7,0.9,0.13,0.5])\n",
        "print(best_yuzhi_aimed_at_1(preda,y_test))\n",
        "#输出\n",
        "(0.7777777777777778, 0.11)#（1类最佳f1值和对应的阈值）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zASVzbOYHMk1"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "def plot_picture(y_test,probas):\n",
        "    # y_test测试集\n",
        "    # probas预测概率\n",
        "    CVD = pd.DataFrame()\n",
        "    CVD['正样本'] = y_test\n",
        "    CVD['score'] = probas\n",
        "    CVD = CVD.sort_values(by='score')\n",
        "    cvd_risk = CVD.reset_index(drop=True)\n",
        "    print(cvd_risk)\n",
        "    H = len(cvd_risk)\n",
        "    h = int(H / 10)\n",
        "    cvd = []\n",
        "    risk = []\n",
        "    h0 = 0\n",
        "    risk_count = 0\n",
        "    cvd_count = 0\n",
        "    for i in range(len(cvd_risk)):\n",
        "        if h0 + h > i + 1:\n",
        "            risk_count = risk_count + cvd_risk.loc[i, \"score\"]\n",
        "            if cvd_risk.loc[i, \"正样本\"] == 1:\n",
        "                cvd_count = cvd_count + 1\n",
        "        else:\n",
        "            h0 = h0 + h\n",
        "            cvd.append(round(cvd_count / h, 3))\n",
        "            risk.append(round(risk_count / h, 3))\n",
        "            risk_count = 0\n",
        "            cvd_count = 0\n",
        "\n",
        "    labels = ['10', '9', '8', '7', '6', '5', '4', '3', '2', '1']\n",
        "    cvd.reverse()\n",
        "\n",
        "    x = np.arange(len(labels))  # the label locations\n",
        "    width = 0.8  # the width of the bars\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    rects1 = ax.bar(x, cvd, width, color='royalblue')\n",
        "    # rects2 = ax.bar(x + width / 2, risk, width, label='Estimated', color='indianred')\n",
        "\n",
        "    # Add some text for labels, title and custom x-axis tick labels, etc.\n",
        "    ax.set_ylabel('Proportion of Positive Samples')\n",
        "    ax.set_xlabel('Decile of Estimated Score')\n",
        "    # ax.set_title('Observed Vs Estimated')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(labels)\n",
        "    ax.legend()\n",
        "    plt.axhline(y=np.mean(cvd),  linestyle='--', color='black')\n",
        "    def autolabel(rects):\n",
        "        \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
        "        for rect in rects:\n",
        "            height = rect.get_height()\n",
        "            ax.annotate('{}'.format(height),\n",
        "                        xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                        xytext=(0, 2),  # 3 points vertical offset\n",
        "                        textcoords=\"offset points\",\n",
        "                        ha='center', va='bottom')\n",
        "\n",
        "    autolabel(rects1)\n",
        "    # autolabel(rects2)\n",
        "    fig.tight_layout()\n",
        "    plt.savefig(\"柱状图.png\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlGq4lnZHMk2"
      },
      "outputs": [],
      "source": [
        "def yuzhi(preda,door=0.1):\n",
        "    predict=[]\n",
        "    for i in range(len(preda)):\n",
        "        if preda[i] < door:\n",
        "            predict.append(0)\n",
        "        else:\n",
        "            predict.append(1)\n",
        "    return predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5SxmEAUHMk2"
      },
      "outputs": [],
      "source": [
        "#按阈值最大\n",
        "from sklearn.metrics import roc_auc_score,roc_curve,auc\n",
        "from numpy import argmax\n",
        "def find_optimal_cutoff(tpr,fpr,threshold):\n",
        "    optimal_idx = np.argmax(tpr - fpr)\n",
        "    optimal_threshold = threshold[optimal_idx]\n",
        "    return optimal_threshold\n",
        "\n",
        "def best_confusion_matrix(y_test, y_test_predprob):\n",
        "    \"\"\"\n",
        "        根据真实值和预测值（预测概率）的向量来计算混淆矩阵和最优的划分阈值\n",
        "\n",
        "        Args:\n",
        "            y_test:真实值\n",
        "            y_test_predprob：预测值\n",
        "\n",
        "        Returns:\n",
        "            返回最佳划分阈值和混淆矩阵\n",
        "        \"\"\"\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, y_test_predprob, pos_label=1)\n",
        "    cutoff = find_optimal_cutoff(tpr,fpr,thresholds)\n",
        "    y_pred = yuzhi(y_test_predprob,cutoff)\n",
        "    print(metrics.confusion_matrix(y_true=y_test, y_pred=y_pred))\n",
        "    print(metrics.classification_report(y_true=y_test, y_pred=y_pred))\n",
        "    TN,FP,FN,TP = metrics.confusion_matrix(y_test,y_pred).ravel()\n",
        "    return cutoff,TN,FN,FP,TP\n",
        "best_confusion_matrix(test_Y_input,preda)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qr32yNIyHMk2"
      },
      "outputs": [],
      "source": [
        "precisions, recalls, thresholds = precision_recall_curve(test_Y_input,predict)\n",
        "\n",
        "# 拿到最优结果以及索引\n",
        "f1_scores = (2 * precisions * recalls) / (precisions + recalls)\n",
        "best_f1_score = np.max(f1_scores[np.isfinite(f1_scores)])\n",
        "best_f1_score_index = np.argmax(f1_scores[np.isfinite(f1_scores)])\n",
        "\n",
        "# 阈值\n",
        "best_f1_score, thresholds[best_f1_score_index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qw7zyYoiZiaI",
        "outputId": "49f393f2-2f40-4461-ffd7-5a869e7a6b79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch_metric_learning\n",
            "  Downloading pytorch_metric_learning-1.5.2-py3-none-any.whl (111 kB)\n",
            "\u001b[K     |████████████████████████████████| 111 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_metric_learning) (4.64.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_metric_learning) (1.21.6)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from pytorch_metric_learning) (0.13.1+cu113)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_metric_learning) (1.12.1+cu113)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pytorch_metric_learning) (1.0.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (4.1.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pytorch_metric_learning) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pytorch_metric_learning) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pytorch_metric_learning) (1.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->pytorch_metric_learning) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->pytorch_metric_learning) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->pytorch_metric_learning) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->pytorch_metric_learning) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->pytorch_metric_learning) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->pytorch_metric_learning) (3.0.4)\n",
            "Installing collected packages: pytorch-metric-learning\n",
            "Successfully installed pytorch-metric-learning-1.5.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch_metric_learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUwm114yjvuD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}