{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9H6Gu5EKHVb5",
        "outputId": "4f40deb0-99e5-4ec8-886b-96f793ac5916"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1lakAphH7Ir",
        "outputId": "ff247bb6-7406-4064-d8bb-dedf043d381e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m97.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n",
            "Collecting pytorch\n",
            "  Downloading pytorch-1.0.2.tar.gz (689 bytes)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pytorch\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for pytorch (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for pytorch\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for pytorch\n",
            "Failed to build pytorch\n",
            "\u001b[31mERROR: Could not build wheels for pytorch, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting sklearn\n",
            "  Downloading sklearn-0.0.post5.tar.gz (3.7 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install pytorch\n",
        "!pip install sklearn\n",
        "!pip install thop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Auk8ljxTwQCJ"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoModel\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self, embed_dim, journal_size):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.bert = AutoModel.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\")\n",
        "\n",
        "        # self.tokenizer = AutoTokenizer.from_pretrained(\"scibert_scivocab_uncased\")\n",
        "        self.atten = nn.MultiheadAttention(embed_dim=embed_dim,num_heads=8,dropout=0.1)\n",
        "        self.liner_query = nn.Linear(embed_dim, embed_dim)\n",
        "        self.liner_key = nn.Linear(embed_dim, embed_dim)\n",
        "        self.liner_value = nn.Linear(embed_dim, embed_dim)\n",
        "        #self.liner1 = nn.Linear(journal_size, journal_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.liner2 = nn.Linear(embed_dim*2, 2)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def get_sentence_feature(self, input_ids):\n",
        "        outputs = self.bert(input_ids)\n",
        "        pooled_output = outputs[1]\n",
        "        # print('pooled_output.shape:',pooled_output.shape)\n",
        "        return pooled_output\n",
        "\n",
        "    def forward(self, journal, title, abasruct):\n",
        "#         print('journal.shape',journal.shape)\n",
        "#         print('title.shape',title.shape)\n",
        "#         print('abasruct.shape',abasruct.shape)\n",
        "        title_vector = self.get_sentence_feature(title).unsqueeze(0)\n",
        "        abasruct_vector = self.get_sentence_feature(abasruct).unsqueeze(0)\n",
        "#         print('title.shape',title_vector.shape)\n",
        "#         print('abasruct.shape',abasruct_vector.shape)\n",
        "        title_query_vector, title_key_vector, title_value_vector = self.liner_query(title_vector),self.liner_key(title_vector),self.liner_value(title_vector)\n",
        "        title_atten,_ = self.atten(title_query_vector, title_key_vector, title_value_vector)\n",
        "        abasruct_query_vector, abasruct_key_vector, abasruct_value_vector = self.liner_query(abasruct_vector),self.liner_key(abasruct_vector),self.liner_value(abasruct_vector)\n",
        "        abasruct_atten,_ = self.atten(abasruct_query_vector, abasruct_key_vector, abasruct_value_vector)\n",
        "        #journal_vector = self.liner1(journal)\n",
        "        #journal_vector = self.relu(journal_vector)\n",
        "        # print('journal_vector.shape',journal_vector.shape)\n",
        "#         print('title_atten.shape',title_atten.shape)\n",
        "#         print('abasruct_atten.shape',abasruct_atten.shape)\n",
        "        feature = torch.cat((title_atten.squeeze(0), abasruct_atten.squeeze(0)), 1)\n",
        "        # feature = torch.cat((journal_vector, title_vector, abasruct_vector), 1)\n",
        "        out = self.liner2(feature)\n",
        "        output = self.softmax(out)\n",
        "        return output\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Rx3JgKEvF6X1"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "def focal_loss(\n",
        "    inputs: torch.Tensor,\n",
        "    targets: torch.Tensor,\n",
        "    alpha: float = 0.80,#0.40\n",
        "    gamma: float = 2,\n",
        "    reduction: str = \"mean\",\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        inputs: A float tensor of arbitrary shape.\n",
        "                The predictions which have been sigmod for each example.\n",
        "        targets: A float tensor with the same shape as inputs. Stores the binary\n",
        "                 classification label for each element in inputs\n",
        "                (0 for the negative class and 1 for the positive class).\n",
        "        alpha: (optional) Weighting factor in range (0,1) to balance\n",
        "                positive vs negative examples. Default = -1 (no weighting).\n",
        "        gamma: Exponent of the modulating factor (1 - p_t) to\n",
        "               balance easy vs hard examples.\n",
        "        reduction: 'none' | 'mean' | 'sum'\n",
        "                 'none': No reduction will be applied to the output.\n",
        "                 'mean': The output will be averaged.\n",
        "                 'sum': The output will be summed.\n",
        "    Returns:\n",
        "        Loss tensor with the reduction option applied.\n",
        "    \"\"\"\n",
        "    inputs = inputs.float()\n",
        "    targets = targets.float()\n",
        "    p = inputs\n",
        "    ce_loss = F.binary_cross_entropy(inputs, targets, reduction=\"none\")\n",
        "    p_t = p * targets + (1 - p) * (1 - targets)\n",
        "    loss = ce_loss * ((1 - p_t) ** gamma)\n",
        "\n",
        "    if alpha >= 0:\n",
        "        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n",
        "        loss = alpha_t * loss\n",
        "\n",
        "    if reduction == \"mean\":\n",
        "        loss = loss.mean()\n",
        "    elif reduction == \"sum\":\n",
        "        loss = loss.sum()\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHqo2jKkwPy6",
        "outputId": "fd507b1c-1603-46e3-8db0-055bec45e8a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i =  0\n",
            "loss =  0.09483034908771515\n",
            "i =  800\n",
            "loss =  0.05771775171160698\n",
            "i =  1600\n",
            "loss =  0.040137022733688354\n",
            "i =  2400\n",
            "loss =  0.09647788107395172\n",
            "evaluating...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epochs: 0\n",
            "PAJO-AT AUC =  0.7391348163234999\n",
            "Specificity =  0.0\n",
            "Sensitivity =  1.0\n",
            "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
            "[INFO] Register count_softmax() for <class 'torch.nn.modules.activation.Softmax'>.\n",
            "Flops: 190444548116.00\n",
            "MLOPS: 9.07\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       469\n",
            "           1     0.3000    1.0000    0.4615       201\n",
            "\n",
            "    accuracy                         0.3000       670\n",
            "   macro avg     0.1500    0.5000    0.2308       670\n",
            "weighted avg     0.0900    0.3000    0.1385       670\n",
            "\n",
            "i =  0\n",
            "loss =  0.06820885092020035\n",
            "i =  800\n",
            "loss =  0.05889814347028732\n",
            "i =  1600\n",
            "loss =  0.025366194546222687\n",
            "i =  2400\n",
            "loss =  0.11695250868797302\n",
            "i =  0\n",
            "loss =  0.05316691845655441\n",
            "i =  800\n",
            "loss =  0.05315239727497101\n",
            "i =  1600\n",
            "loss =  0.057948037981987\n",
            "i =  2400\n",
            "loss =  0.08563542366027832\n",
            "i =  0\n",
            "loss =  0.06116434186697006\n",
            "i =  800\n",
            "loss =  0.0446239709854126\n",
            "i =  1600\n",
            "loss =  0.04052798077464104\n",
            "i =  2400\n",
            "loss =  0.049008578062057495\n",
            "evaluating...\n",
            "epochs: 3\n",
            "PAJO-AT AUC =  0.8671779694279137\n",
            "Specificity =  0.6375266524520256\n",
            "Sensitivity =  0.9104477611940298\n",
            "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
            "[INFO] Register count_softmax() for <class 'torch.nn.modules.activation.Softmax'>.\n",
            "Flops: 190444548116.00\n",
            "MLOPS: 9.08\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9432    0.6375    0.7608       469\n",
            "           1     0.5184    0.9104    0.6606       201\n",
            "\n",
            "    accuracy                         0.7194       670\n",
            "   macro avg     0.7308    0.7740    0.7107       670\n",
            "weighted avg     0.8158    0.7194    0.7308       670\n",
            "\n",
            "i =  0\n",
            "loss =  0.016922837123274803\n",
            "i =  800\n",
            "loss =  0.042247477918863297\n",
            "i =  1600\n",
            "loss =  0.006883964408189058\n",
            "i =  2400\n",
            "loss =  0.05269283801317215\n",
            "i =  0\n",
            "loss =  0.011979600414633751\n",
            "i =  800\n",
            "loss =  0.04094983637332916\n",
            "i =  1600\n",
            "loss =  0.0045478735119104385\n",
            "i =  2400\n",
            "loss =  0.03070468083024025\n",
            "i =  0\n",
            "loss =  0.05463440716266632\n",
            "i =  800\n",
            "loss =  0.03789713978767395\n",
            "i =  1600\n",
            "loss =  0.011476974003016949\n",
            "i =  2400\n",
            "loss =  0.025003526359796524\n",
            "evaluating...\n",
            "epochs: 6\n",
            "PAJO-AT AUC =  0.8798544590480435\n",
            "Specificity =  0.9189765458422174\n",
            "Sensitivity =  0.6368159203980099\n",
            "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
            "[INFO] Register count_softmax() for <class 'torch.nn.modules.activation.Softmax'>.\n",
            "Flops: 190444548116.00\n",
            "MLOPS: 9.08\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8552    0.9190    0.8859       469\n",
            "           1     0.7711    0.6368    0.6975       201\n",
            "\n",
            "    accuracy                         0.8343       670\n",
            "   macro avg     0.8131    0.7779    0.7917       670\n",
            "weighted avg     0.8299    0.8343    0.8294       670\n",
            "\n",
            "i =  0\n",
            "loss =  0.01713232323527336\n",
            "i =  800\n",
            "loss =  0.06039392948150635\n",
            "i =  1600\n",
            "loss =  0.01964706741273403\n",
            "i =  2400\n",
            "loss =  0.03018523007631302\n",
            "i =  0\n",
            "loss =  0.0434609055519104\n",
            "i =  800\n",
            "loss =  0.011317646130919456\n",
            "i =  1600\n",
            "loss =  0.003803311148658395\n",
            "i =  2400\n",
            "loss =  0.011639142408967018\n",
            "i =  0\n",
            "loss =  0.0019119980279356241\n",
            "i =  800\n",
            "loss =  0.05727728083729744\n",
            "i =  1600\n",
            "loss =  0.015174897387623787\n",
            "i =  2400\n",
            "loss =  0.03364871069788933\n",
            "evaluating...\n",
            "epochs: 9\n",
            "PAJO-AT AUC =  0.7802936278097784\n",
            "Specificity =  0.8272921108742004\n",
            "Sensitivity =  0.6467661691542289\n",
            "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
            "[INFO] Register count_softmax() for <class 'torch.nn.modules.activation.Softmax'>.\n",
            "Flops: 190444548116.00\n",
            "MLOPS: 9.09\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8453    0.8273    0.8362       469\n",
            "           1     0.6161    0.6468    0.6311       201\n",
            "\n",
            "    accuracy                         0.7731       670\n",
            "   macro avg     0.7307    0.7370    0.7336       670\n",
            "weighted avg     0.7766    0.7731    0.7747       670\n",
            "\n",
            "i =  0\n",
            "loss =  0.011188668198883533\n",
            "i =  800\n",
            "loss =  0.005439985077828169\n",
            "i =  1600\n",
            "loss =  0.010943353176116943\n",
            "i =  2400\n",
            "loss =  0.02263784408569336\n",
            "i =  0\n",
            "loss =  0.0038303222972899675\n",
            "i =  800\n",
            "loss =  0.007902569137513638\n",
            "i =  1600\n",
            "loss =  0.013363535515964031\n",
            "i =  2400\n",
            "loss =  0.006082171108573675\n",
            "i =  0\n",
            "loss =  0.0673987939953804\n",
            "i =  800\n",
            "loss =  0.05995689332485199\n",
            "i =  1600\n",
            "loss =  0.04110915958881378\n",
            "i =  2400\n",
            "loss =  0.06984249502420425\n",
            "evaluating...\n",
            "epochs: 12\n",
            "PAJO-AT AUC =  0.7819219467693517\n",
            "Specificity =  0.8933901918976546\n",
            "Sensitivity =  0.6368159203980099\n",
            "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
            "[INFO] Register count_softmax() for <class 'torch.nn.modules.activation.Softmax'>.\n",
            "Flops: 190444548116.00\n",
            "MLOPS: 9.09\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8516    0.8934    0.8720       469\n",
            "           1     0.7191    0.6368    0.6755       201\n",
            "\n",
            "    accuracy                         0.8164       670\n",
            "   macro avg     0.7854    0.7651    0.7737       670\n",
            "weighted avg     0.8119    0.8164    0.8130       670\n",
            "\n",
            "i =  0\n",
            "loss =  0.006370695307850838\n",
            "i =  800\n",
            "loss =  0.042538635432720184\n",
            "i =  1600\n",
            "loss =  0.012929288670420647\n",
            "i =  2400\n",
            "loss =  0.019107891246676445\n",
            "i =  0\n",
            "loss =  0.0034295266959816217\n",
            "i =  800\n",
            "loss =  0.03903890773653984\n",
            "i =  1600\n",
            "loss =  0.07872512936592102\n",
            "i =  2400\n",
            "loss =  0.010184830985963345\n",
            "i =  0\n",
            "loss =  0.011111225001513958\n",
            "i =  800\n",
            "loss =  0.07140430063009262\n",
            "i =  1600\n",
            "loss =  0.02258117124438286\n",
            "i =  2400\n",
            "loss =  0.07303869724273682\n",
            "evaluating...\n",
            "epochs: 15\n",
            "PAJO-AT AUC =  0.7867114321781286\n",
            "Specificity =  0.39232409381663114\n",
            "Sensitivity =  0.9502487562189055\n",
            "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
            "[INFO] Register count_softmax() for <class 'torch.nn.modules.activation.Softmax'>.\n",
            "Flops: 190444548116.00\n",
            "MLOPS: 9.10\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9485    0.3923    0.5551       469\n",
            "           1     0.4013    0.9502    0.5643       201\n",
            "\n",
            "    accuracy                         0.5597       670\n",
            "   macro avg     0.6749    0.6713    0.5597       670\n",
            "weighted avg     0.7843    0.5597    0.5578       670\n",
            "\n",
            "i =  0\n",
            "loss =  0.01731240004301071\n",
            "i =  800\n",
            "loss =  0.06433717161417007\n",
            "i =  1600\n",
            "loss =  0.013917135074734688\n",
            "i =  2400\n",
            "loss =  0.07272441685199738\n",
            "i =  0\n",
            "loss =  0.059984657913446426\n",
            "i =  800\n",
            "loss =  0.042198535054922104\n",
            "i =  1600\n",
            "loss =  0.02982867881655693\n",
            "i =  2400\n",
            "loss =  0.07448961585760117\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from thop import profile\n",
        "import time\n",
        "\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "#device = 'cpu'\n",
        "\n",
        "batch_size = 4\n",
        "lr = 3e-5\n",
        "EPOCHS = 18\n",
        "\n",
        "def shuffle_dataset(journal, title, abstruct, label):\n",
        "    length = len(journal)\n",
        "    rng = np.random.default_rng(12345)\n",
        "    index = np.arange(length)\n",
        "    # print(index)\n",
        "    rng.shuffle(index)\n",
        "    # print(index)\n",
        "    return journal[index], title[index], abstruct[index], label[index]\n",
        "\n",
        "def evaluate(model, test_journal, test_title, test_abstruct, test_Y):\n",
        "    model.eval()\n",
        "    pred = []\n",
        "    logits = []\n",
        "    true_Y = []\n",
        "    length = len(test_Y)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(test_Y), batch_size):\n",
        "        # for i in range(0, 10, batch_size):\n",
        "            logit = model(test_journal[i:min(len(test_Y),i+batch_size)], test_title[i:min(len(test_Y),i+batch_size)], test_abstruct[i:min(len(test_Y),i+batch_size)]).cpu()\n",
        "            logits.extend(logit)\n",
        "            y_pred = torch.argmax(logit, dim=1).cpu()\n",
        "            pred.extend(y_pred)\n",
        "            true_Y.extend(test_Y[i:min(len(test_Y),i+batch_size)].cpu())\n",
        "        # print(true_Y)\n",
        "        # print(logits)\n",
        "        # print(pred)\n",
        "        true_label = []\n",
        "        prob = []\n",
        "        pred_label = []\n",
        "        for i in range(len(true_Y)):\n",
        "          true_label.append(true_Y[i].item())\n",
        "          prob.append(logits[i][1].item())\n",
        "          pred_label.append(pred[i].item())\n",
        "        # print(true_label)\n",
        "        # print(prob)\n",
        "        # print(pred_label)\n",
        "\n",
        "        # 计算specificity、sensitivity\n",
        "        tn, fp, fn, tp = confusion_matrix(true_label, pred_label).ravel()\n",
        "        specificity = tn / (tn + fp)\n",
        "        sensitivity = tp / (tp + fn)\n",
        "\n",
        "    return {\n",
        "        'label':true_label, 'proba':prob,\n",
        "        'AUC':roc_auc_score(true_label, prob),\n",
        "        'classification_report':classification_report(true_label, pred_label, digits=4),\n",
        "        'specificity': specificity,\n",
        "        'sensitivity': sensitivity\n",
        "    }\n",
        "\n",
        "\n",
        "def main():\n",
        "    # 按0和1生成训练集和测试集\n",
        "    X_0 = pd.read_excel('/content/drive/MyDrive/pajo_data/X_0.xlsx').values\n",
        "    Y_0 = pd.read_excel('/content/drive/MyDrive/pajo_data/Y_0.xlsx').values\n",
        "    X_1 = pd.read_excel('/content/drive/MyDrive/pajo_data/X_1.xlsx').values\n",
        "    Y_1 = pd.read_excel('/content/drive/MyDrive/pajo_data/Y_1.xlsx').values\n",
        "    title_0 = np.load('/content/drive/MyDrive/pajo_data/token_title_0.npy')\n",
        "    title_1 = np.load('/content/drive/MyDrive/pajo_data/token_title_1.npy')\n",
        "    abstruct_0 = np.load('/content/drive/MyDrive/pajo_data/token_abstruct_0_512.npy')\n",
        "    abstruct_1 = np.load('/content/drive/MyDrive/pajo_data/token_abstruct_1_512.npy')\n",
        "\n",
        "    rs = np.random.RandomState(42)\n",
        "    L = list(rs.randint(0, len(X_0), int(7/3*len(X_1))))\n",
        "    X_0 = X_0[L]\n",
        "    Y_0 = Y_0[L]\n",
        "    title_0=title_0[L]\n",
        "    abstruct_0 = abstruct_0[L]\n",
        "\n",
        "    journal_train_X_0, journal_test_X_0, train_Y_0,test_Y_0 = train_test_split(X_0, Y_0, train_size=0.80, random_state=42)\n",
        "    journal_train_X_1, journal_test_X_1, train_Y_1, test_Y_1 = train_test_split(X_1, Y_1, train_size=0.80, random_state=42)\n",
        "    title_train_X_0, title_test_X_0, _,_ = train_test_split(title_0, Y_0, train_size=0.80, random_state=42)\n",
        "    title_train_X_1, title_test_X_1, _,_ = train_test_split(title_1, Y_1, train_size=0.80, random_state=42)\n",
        "    abstruct_train_X_0, abstruct_test_X_0, _,_ = train_test_split(abstruct_0, Y_0, train_size=0.80, random_state=42)\n",
        "    abstruct_train_X_1, abstruct_test_X_1, _,_ = train_test_split(abstruct_1, Y_1, train_size=0.80, random_state=42)\n",
        "\n",
        "    test_journal = torch.from_numpy(np.vstack((journal_test_X_1, journal_test_X_0))).float().to(device)\n",
        "    test_title = torch.from_numpy(np.vstack((title_test_X_1, title_test_X_0))).to(device)\n",
        "    test_abstruct = torch.from_numpy(np.vstack((abstruct_test_X_1, abstruct_test_X_0))).to(device)\n",
        "    test_Y = torch.from_numpy(np.vstack((test_Y_1, test_Y_0))).to(device)\n",
        "    test_journal, test_title, test_abstruct, test_Y = shuffle_dataset(test_journal, test_title, test_abstruct, test_Y)\n",
        "\n",
        "\n",
        "    model = MyModel(embed_dim=768, journal_size=test_journal.shape[1]).to(device)\n",
        "#     model.load_state_dict(torch.load(\"./res/new_model_2_0.75.pt\"))\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "    journal_train_X_0 = torch.from_numpy(journal_train_X_0).float().to(device)\n",
        "    title_train_X_0 = torch.from_numpy(title_train_X_0).to(device)\n",
        "    abstruct_train_X_0 = torch.from_numpy(abstruct_train_X_0).to(device)\n",
        "    train_Y_0 = torch.from_numpy(train_Y_0).to(device)\n",
        "\n",
        "    journal_train_X_1 = torch.from_numpy(journal_train_X_1).float().to(device)\n",
        "    title_train_X_1 = torch.from_numpy(title_train_X_1).to(device)\n",
        "    abstruct_train_X_1 = torch.from_numpy(abstruct_train_X_1).to(device)\n",
        "    train_Y_1 = torch.from_numpy(train_Y_1).to(device)\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        start_time = time.time()\n",
        "        journal_train_X_0, title_train_X_0, abstruct_train_X_0, train_Y_0 = shuffle_dataset(journal_train_X_0, title_train_X_0, abstruct_train_X_0, train_Y_0)\n",
        "        train_journal = torch.cat((journal_train_X_1, journal_train_X_0))\n",
        "        # print('journal_train_X_1.shape:',journal_train_X_1.shape)\n",
        "        train_title = torch.cat((title_train_X_1, title_train_X_0))\n",
        "        train_abstruct = torch.cat((abstruct_train_X_1, abstruct_train_X_0))\n",
        "        train_Y = torch.cat((train_Y_1, train_Y_0))\n",
        "        train_journal, train_title, train_abstruct, train_Y = shuffle_dataset(train_journal, train_title, train_abstruct, train_Y)\n",
        "\n",
        "        for i in range(0, len(train_Y), batch_size):\n",
        "        #for i in range(0, 10, batch_size):\n",
        "            model.zero_grad()\n",
        "            logits = model(train_journal[i:min(len(train_Y),i+batch_size)], train_title[i:min(len(train_Y),i+batch_size)], train_abstruct[i:min(len(train_Y),i+batch_size)])\n",
        "            loss = focal_loss(logits[:,1].unsqueeze(1), train_Y[i:i+batch_size])\n",
        "            loss.backward()\n",
        "            if((i/batch_size)%200 == 0):\n",
        "              print('i = ', i)\n",
        "              print('loss = ', loss.item())\n",
        "            optimizer.step()\n",
        "\n",
        "        end_time = time.time()\n",
        "\n",
        "        # 计算每秒钟的训练样本数（即MLOPS）\n",
        "        num_samples = len(train_Y)\n",
        "        training_time = end_time - start_time\n",
        "        samples_per_second = num_samples / training_time\n",
        "\n",
        "        if(epoch%3 == 0):\n",
        "\n",
        "              print('evaluating...')\n",
        "\n",
        "              val_metrics = evaluate(model, test_journal, test_title, test_abstruct, test_Y)\n",
        "\n",
        "              print('epochs:',epoch)\n",
        "              print('PAJO-AT AUC = ',val_metrics['AUC'])\n",
        "              print('Specificity = ', val_metrics['specificity'])\n",
        "              print('Sensitivity = ', val_metrics['sensitivity'])\n",
        "              # 输出模型信息\n",
        "              flops, params = profile(model, inputs=(train_journal[0:batch_size], train_title[0:batch_size], train_abstruct[0:batch_size]))\n",
        "              print(\"Flops: {:.2f}\".format(flops))\n",
        "              print(\"MLOPS: {:.2f}\".format(samples_per_second))\n",
        "              print(val_metrics['classification_report'])\n",
        "\n",
        "              torch.save(model.state_dict(), './res/no_key_2_0.80_{}.pt'.format(epoch))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    if not os.path.exists('./res'):\n",
        "        os.mkdir('./res')\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -V"
      ],
      "metadata": {
        "id": "fa25Fjh3RHwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)  #注意是双下划线"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CLQNlJyRMMk",
        "outputId": "a1c564c6-93df-4115-8b77-7000c7eb4faa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.1+cu118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZWnARFxMp1j",
        "outputId": "fca6357c-d12c-454d-897d-3b0f34b70504"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  data.zip\n",
            "warning [data.zip]:  494025 extra bytes at beginning or within zipfile\n",
            "  (attempting to process anyway)\n",
            "file #1:  bad zipfile offset (local header sig):  494025\n",
            "  (attempting to re-compensate)\n",
            "   creating: data/\n",
            "error: invalid zip file with overlapped components (possible zip bomb)\n"
          ]
        }
      ],
      "source": [
        "!unzip data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KiD2RqnBHMk1"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings(action = 'ignore')\n",
        "\n",
        "def best_yuzhi_aimed_at_1(preda,y_test):\n",
        "    #preda为预测为1类的概率,输入形式为narray\n",
        "    precisions = []\n",
        "    recalls = []\n",
        "    for i in np.arange(0.01,1,0.01):\n",
        "        y_pred = np.where(preda<i,0,1)\n",
        "        TN,FP,FN,TP = metrics.confusion_matrix(y_test,y_pred).ravel()\n",
        "        precision = TP/(TP+FP)\n",
        "        recall = TP/(TP+FN)\n",
        "        precisions.append(precision)\n",
        "        recalls.append(recall)\n",
        "    precisions = np.array(precisions)\n",
        "    recalls = np.array(recalls)\n",
        "    f1_scores = (2 * precisions * recalls) / (precisions + recalls)\n",
        "    best_f1_score = np.max(f1_scores[np.isfinite(f1_scores)])\n",
        "    best_f1_score_index = np.argmax(f1_scores[np.isfinite(f1_scores)])\n",
        "    return best_f1_score, np.arange(0.01,1,0.01)[best_f1_score_index]\n",
        "#调用示例\n",
        "y_test=np.array([0,0,1,1,1,0,1,0,1,0,1,1])\n",
        "preda = np.array([0.2,0.3,0.4,0.44,0.45,0.56,0.3,0.1,0.7,0.9,0.13,0.5])\n",
        "print(best_yuzhi_aimed_at_1(preda,y_test))\n",
        "#输出\n",
        "(0.7777777777777778, 0.11)#（1类最佳f1值和对应的阈值）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zASVzbOYHMk1"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "def plot_picture(y_test,probas):\n",
        "    # y_test测试集\n",
        "    # probas预测概率\n",
        "    CVD = pd.DataFrame()\n",
        "    CVD['正样本'] = y_test\n",
        "    CVD['score'] = probas\n",
        "    CVD = CVD.sort_values(by='score')\n",
        "    cvd_risk = CVD.reset_index(drop=True)\n",
        "    print(cvd_risk)\n",
        "    H = len(cvd_risk)\n",
        "    h = int(H / 10)\n",
        "    cvd = []\n",
        "    risk = []\n",
        "    h0 = 0\n",
        "    risk_count = 0\n",
        "    cvd_count = 0\n",
        "    for i in range(len(cvd_risk)):\n",
        "        if h0 + h > i + 1:\n",
        "            risk_count = risk_count + cvd_risk.loc[i, \"score\"]\n",
        "            if cvd_risk.loc[i, \"正样本\"] == 1:\n",
        "                cvd_count = cvd_count + 1\n",
        "        else:\n",
        "            h0 = h0 + h\n",
        "            cvd.append(round(cvd_count / h, 3))\n",
        "            risk.append(round(risk_count / h, 3))\n",
        "            risk_count = 0\n",
        "            cvd_count = 0\n",
        "\n",
        "    labels = ['10', '9', '8', '7', '6', '5', '4', '3', '2', '1']\n",
        "    cvd.reverse()\n",
        "\n",
        "    x = np.arange(len(labels))  # the label locations\n",
        "    width = 0.8  # the width of the bars\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    rects1 = ax.bar(x, cvd, width, color='royalblue')\n",
        "    # rects2 = ax.bar(x + width / 2, risk, width, label='Estimated', color='indianred')\n",
        "\n",
        "    # Add some text for labels, title and custom x-axis tick labels, etc.\n",
        "    ax.set_ylabel('Proportion of Positive Samples')\n",
        "    ax.set_xlabel('Decile of Estimated Score')\n",
        "    # ax.set_title('Observed Vs Estimated')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(labels)\n",
        "    ax.legend()\n",
        "    plt.axhline(y=np.mean(cvd),  linestyle='--', color='black')\n",
        "    def autolabel(rects):\n",
        "        \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
        "        for rect in rects:\n",
        "            height = rect.get_height()\n",
        "            ax.annotate('{}'.format(height),\n",
        "                        xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                        xytext=(0, 2),  # 3 points vertical offset\n",
        "                        textcoords=\"offset points\",\n",
        "                        ha='center', va='bottom')\n",
        "\n",
        "    autolabel(rects1)\n",
        "    # autolabel(rects2)\n",
        "    fig.tight_layout()\n",
        "    plt.savefig(\"柱状图.png\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlGq4lnZHMk2"
      },
      "outputs": [],
      "source": [
        "def yuzhi(preda,door=0.1):\n",
        "    predict=[]\n",
        "    for i in range(len(preda)):\n",
        "        if preda[i] < door:\n",
        "            predict.append(0)\n",
        "        else:\n",
        "            predict.append(1)\n",
        "    return predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5SxmEAUHMk2"
      },
      "outputs": [],
      "source": [
        "#按阈值最大\n",
        "from sklearn.metrics import roc_auc_score,roc_curve,auc\n",
        "from numpy import argmax\n",
        "def find_optimal_cutoff(tpr,fpr,threshold):\n",
        "    optimal_idx = np.argmax(tpr - fpr)\n",
        "    optimal_threshold = threshold[optimal_idx]\n",
        "    return optimal_threshold\n",
        "\n",
        "def best_confusion_matrix(y_test, y_test_predprob):\n",
        "    \"\"\"\n",
        "        根据真实值和预测值（预测概率）的向量来计算混淆矩阵和最优的划分阈值\n",
        "\n",
        "        Args:\n",
        "            y_test:真实值\n",
        "            y_test_predprob：预测值\n",
        "\n",
        "        Returns:\n",
        "            返回最佳划分阈值和混淆矩阵\n",
        "        \"\"\"\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, y_test_predprob, pos_label=1)\n",
        "    cutoff = find_optimal_cutoff(tpr,fpr,thresholds)\n",
        "    y_pred = yuzhi(y_test_predprob,cutoff)\n",
        "    print(metrics.confusion_matrix(y_true=y_test, y_pred=y_pred))\n",
        "    print(metrics.classification_report(y_true=y_test, y_pred=y_pred))\n",
        "    TN,FP,FN,TP = metrics.confusion_matrix(y_test,y_pred).ravel()\n",
        "    return cutoff,TN,FN,FP,TP\n",
        "best_confusion_matrix(test_Y_input,preda)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qr32yNIyHMk2"
      },
      "outputs": [],
      "source": [
        "precisions, recalls, thresholds = precision_recall_curve(test_Y_input,predict)\n",
        "\n",
        "# 拿到最优结果以及索引\n",
        "f1_scores = (2 * precisions * recalls) / (precisions + recalls)\n",
        "best_f1_score = np.max(f1_scores[np.isfinite(f1_scores)])\n",
        "best_f1_score_index = np.argmax(f1_scores[np.isfinite(f1_scores)])\n",
        "\n",
        "# 阈值\n",
        "best_f1_score, thresholds[best_f1_score_index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qw7zyYoiZiaI",
        "outputId": "49f393f2-2f40-4461-ffd7-5a869e7a6b79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch_metric_learning\n",
            "  Downloading pytorch_metric_learning-1.5.2-py3-none-any.whl (111 kB)\n",
            "\u001b[K     |████████████████████████████████| 111 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_metric_learning) (4.64.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_metric_learning) (1.21.6)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from pytorch_metric_learning) (0.13.1+cu113)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_metric_learning) (1.12.1+cu113)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pytorch_metric_learning) (1.0.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (4.1.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pytorch_metric_learning) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pytorch_metric_learning) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pytorch_metric_learning) (1.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->pytorch_metric_learning) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->pytorch_metric_learning) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->pytorch_metric_learning) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->pytorch_metric_learning) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->pytorch_metric_learning) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->pytorch_metric_learning) (3.0.4)\n",
            "Installing collected packages: pytorch-metric-learning\n",
            "Successfully installed pytorch-metric-learning-1.5.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch_metric_learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUwm114yjvuD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}