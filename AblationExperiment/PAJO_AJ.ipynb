{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFPuvth28NSK",
        "outputId": "0236d8c4-0e72-40c1-fc7f-8d8d23f21a32"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install pytorch\n",
        "!pip install sklearn\n",
        "!pip install thop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9bjDFZ98O8d",
        "outputId": "5bf8a421-d10f-4a05-a37b-9900f82c089b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n",
            "Collecting pytorch\n",
            "  Downloading pytorch-1.0.2.tar.gz (689 bytes)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pytorch\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for pytorch (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for pytorch\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for pytorch\n",
            "Failed to build pytorch\n",
            "\u001b[31mERROR: Could not build wheels for pytorch, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting sklearn\n",
            "  Downloading sklearn-0.0.post5.tar.gz (3.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: sklearn\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0.post5-py3-none-any.whl size=2950 sha256=121a58441f527065c33728e7c87958d1017b2d15746eddf5dfa3e9a881d5d54e\n",
            "  Stored in directory: /root/.cache/pip/wheels/38/1f/8d/4f812c590e074c1e928f5cec67bf5053b71f38e2648739403a\n",
            "Successfully built sklearn\n",
            "Installing collected packages: sklearn\n",
            "Successfully installed sklearn-0.0.post5\n",
            "Collecting thop\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from thop) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->thop) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->thop) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->thop) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->thop) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->thop) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->thop) (1.3.0)\n",
            "Installing collected packages: thop\n",
            "Successfully installed thop-0.1.1.post2209072238\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Auk8ljxTwQCJ"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoModel\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self, embed_dim, journal_size):\n",
        "        super(MyModel, self).__init__()\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\",padding ='max_length',max_length = 512,truncation=True)\n",
        "        self.bert = AutoModel.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\")\n",
        "\n",
        "\n",
        "        # self.tokenizer = AutoTokenizer.from_pretrained(\"scibert_scivocab_uncased\")\n",
        "        self.atten = nn.MultiheadAttention(embed_dim=embed_dim,num_heads=8,dropout=0.1)\n",
        "        self.liner_query = nn.Linear(embed_dim, embed_dim)\n",
        "        self.liner_key = nn.Linear(embed_dim, embed_dim)\n",
        "        self.liner_value = nn.Linear(embed_dim, embed_dim)\n",
        "        self.liner1 = nn.Linear(journal_size, journal_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.liner2 = nn.Linear(journal_size+embed_dim, 2)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def get_sentence_feature(self, input_ids):\n",
        "        outputs = self.bert(input_ids)\n",
        "        pooled_output = outputs[1]\n",
        "        # print('pooled_output.shape:',pooled_output.shape)\n",
        "        return pooled_output\n",
        "\n",
        "    def forward(self, journal, title, abasruct):\n",
        "#         print('journal.shape',journal.shape)\n",
        "#         print('title.shape',title.shape)\n",
        "#         print('abasruct.shape',abasruct.shape)\n",
        "#         title_vector = self.get_sentence_feature(title).unsqueeze(0)\n",
        "        abasruct_vector = self.get_sentence_feature(abasruct).unsqueeze(0)\n",
        "#         print('title.shape',title_vector.shape)\n",
        "#         print('abasruct.shape',abasruct_vector.shape)\n",
        "#         title_query_vector, title_key_vector, title_value_vector = self.liner_query(title_vector),self.liner_key(title_vector),self.liner_value(title_vector)\n",
        "#         title_atten,_ = self.atten(title_query_vector, title_key_vector, title_value_vector)\n",
        "        abasruct_query_vector, abasruct_key_vector, abasruct_value_vector = self.liner_query(abasruct_vector),self.liner_key(abasruct_vector),self.liner_value(abasruct_vector)\n",
        "        abasruct_atten,_ = self.atten(abasruct_query_vector, abasruct_key_vector, abasruct_value_vector)\n",
        "        journal_vector = self.liner1(journal)\n",
        "        journal_vector = self.relu(journal_vector)\n",
        "        # print('journal_vector.shape',journal_vector.shape)\n",
        "#         print('title_atten.shape',title_atten.shape)\n",
        "#         print('abasruct_atten.shape',abasruct_atten.shape)\n",
        "        feature = torch.cat((journal_vector,abasruct_atten.squeeze(0)), 1)\n",
        "        # feature = torch.cat((journal_vector, title_vector, abasruct_vector), 1)\n",
        "        out = self.liner2(feature)\n",
        "        output = self.softmax(out)\n",
        "        return output\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Rx3JgKEvF6X1"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "def focal_loss(\n",
        "    inputs: torch.Tensor,\n",
        "    targets: torch.Tensor,\n",
        "    alpha: float = 0.80,#0.40\n",
        "    gamma: float = 2,\n",
        "    reduction: str = \"mean\",\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        inputs: A float tensor of arbitrary shape.\n",
        "                The predictions which have been sigmod for each example.\n",
        "        targets: A float tensor with the same shape as inputs. Stores the binary\n",
        "                 classification label for each element in inputs\n",
        "                (0 for the negative class and 1 for the positive class).\n",
        "        alpha: (optional) Weighting factor in range (0,1) to balance\n",
        "                positive vs negative examples. Default = -1 (no weighting).\n",
        "        gamma: Exponent of the modulating factor (1 - p_t) to\n",
        "               balance easy vs hard examples.\n",
        "        reduction: 'none' | 'mean' | 'sum'\n",
        "                 'none': No reduction will be applied to the output.\n",
        "                 'mean': The output will be averaged.\n",
        "                 'sum': The output will be summed.\n",
        "    Returns:\n",
        "        Loss tensor with the reduction option applied.\n",
        "    \"\"\"\n",
        "    inputs = inputs.float()\n",
        "    targets = targets.float()\n",
        "    p = inputs\n",
        "    ce_loss = F.binary_cross_entropy(inputs, targets, reduction=\"none\")\n",
        "    p_t = p * targets + (1 - p) * (1 - targets)\n",
        "    loss = ce_loss * ((1 - p_t) ** gamma)\n",
        "\n",
        "    if alpha >= 0:\n",
        "        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n",
        "        loss = alpha_t * loss\n",
        "\n",
        "    if reduction == \"mean\":\n",
        "        loss = loss.mean()\n",
        "    elif reduction == \"sum\":\n",
        "        loss = loss.sum()\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHqo2jKkwPy6",
        "outputId": "18be8f2b-8a53-4cda-8cfb-8671323b63bc"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i =  0\n",
            "loss =  0.08098867535591125\n",
            "i =  320\n",
            "loss =  0.06670258939266205\n",
            "i =  640\n",
            "loss =  0.05806796997785568\n",
            "i =  960\n",
            "loss =  0.05874984711408615\n",
            "i =  1280\n",
            "loss =  0.03966303914785385\n",
            "i =  1600\n",
            "loss =  0.05389435961842537\n",
            "i =  1920\n",
            "loss =  0.03696205094456673\n",
            "i =  2240\n",
            "loss =  0.03640926256775856\n",
            "i =  2560\n",
            "loss =  0.03566804155707359\n",
            "evaluating...\n",
            "epochs: 0\n",
            "PAJO-AJ AUC =  0.8851690375415034\n",
            "Specificity =  0.8038379530916845\n",
            "Sensitivity =  0.8258706467661692\n",
            "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
            "[INFO] Register count_softmax() for <class 'torch.nn.modules.activation.Softmax'>.\n",
            "Flops: 696451625552.00\n",
            "MLOPS: 9.88\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9150    0.8038    0.8558       469\n",
            "           1     0.6434    0.8259    0.7233       201\n",
            "\n",
            "    accuracy                         0.8104       670\n",
            "   macro avg     0.7792    0.8149    0.7896       670\n",
            "weighted avg     0.8336    0.8104    0.8161       670\n",
            "\n",
            "i =  0\n",
            "loss =  0.02907143160700798\n",
            "i =  320\n",
            "loss =  0.031115535646677017\n",
            "i =  640\n",
            "loss =  0.026778506115078926\n",
            "i =  960\n",
            "loss =  0.052155978977680206\n",
            "i =  1280\n",
            "loss =  0.023877199739217758\n",
            "i =  1600\n",
            "loss =  0.04072745144367218\n",
            "i =  1920\n",
            "loss =  0.02579229511320591\n",
            "i =  2240\n",
            "loss =  0.036856722086668015\n",
            "i =  2560\n",
            "loss =  0.03513915464282036\n",
            "i =  0\n",
            "loss =  0.023605523630976677\n",
            "i =  320\n",
            "loss =  0.03406766802072525\n",
            "i =  640\n",
            "loss =  0.04764660447835922\n",
            "i =  960\n",
            "loss =  0.03666885942220688\n",
            "i =  1280\n",
            "loss =  0.024656761437654495\n",
            "i =  1600\n",
            "loss =  0.04039992392063141\n",
            "i =  1920\n",
            "loss =  0.015372902154922485\n",
            "i =  2240\n",
            "loss =  0.01871015504002571\n",
            "i =  2560\n",
            "loss =  0.01555866189301014\n",
            "i =  0\n",
            "loss =  0.023197680711746216\n",
            "i =  320\n",
            "loss =  0.029613956809043884\n",
            "i =  640\n",
            "loss =  0.04243927076458931\n",
            "i =  960\n",
            "loss =  0.041148148477077484\n",
            "i =  1280\n",
            "loss =  0.01335621066391468\n",
            "i =  1600\n",
            "loss =  0.016361474990844727\n",
            "i =  1920\n",
            "loss =  0.010466417297720909\n",
            "i =  2240\n",
            "loss =  0.014847662299871445\n",
            "i =  2560\n",
            "loss =  0.00933002308011055\n",
            "evaluating...\n",
            "epochs: 3\n",
            "PAJO-AJ AUC =  0.908612587382915\n",
            "Specificity =  0.837953091684435\n",
            "Sensitivity =  0.835820895522388\n",
            "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
            "[INFO] Register count_softmax() for <class 'torch.nn.modules.activation.Softmax'>.\n",
            "Flops: 696451625552.00\n",
            "MLOPS: 9.90\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9225    0.8380    0.8782       469\n",
            "           1     0.6885    0.8358    0.7551       201\n",
            "\n",
            "    accuracy                         0.8373       670\n",
            "   macro avg     0.8055    0.8369    0.8166       670\n",
            "weighted avg     0.8523    0.8373    0.8413       670\n",
            "\n",
            "i =  0\n",
            "loss =  0.004312844946980476\n",
            "i =  320\n",
            "loss =  0.0013564701657742262\n",
            "i =  640\n",
            "loss =  0.016309358179569244\n",
            "i =  960\n",
            "loss =  0.018999088555574417\n",
            "i =  1280\n",
            "loss =  0.009800644591450691\n",
            "i =  1600\n",
            "loss =  0.0006443910533562303\n",
            "i =  1920\n",
            "loss =  0.0171577837318182\n",
            "i =  2240\n",
            "loss =  0.0071871643885970116\n",
            "i =  2560\n",
            "loss =  0.0030167107470333576\n",
            "i =  0\n",
            "loss =  0.0066572632640600204\n",
            "i =  320\n",
            "loss =  0.03257555514574051\n",
            "i =  640\n",
            "loss =  0.000321203435305506\n",
            "i =  960\n",
            "loss =  0.0025227568112313747\n",
            "i =  1280\n",
            "loss =  0.01285669207572937\n",
            "i =  1600\n",
            "loss =  0.012523824349045753\n",
            "i =  1920\n",
            "loss =  0.003995381761342287\n",
            "i =  2240\n",
            "loss =  0.027255740016698837\n",
            "i =  2560\n",
            "loss =  0.034049566835165024\n",
            "i =  0\n",
            "loss =  0.010757876560091972\n",
            "i =  320\n",
            "loss =  0.020231623202562332\n",
            "i =  640\n",
            "loss =  0.005060065072029829\n",
            "i =  960\n",
            "loss =  0.04175812378525734\n",
            "i =  1280\n",
            "loss =  0.004111174494028091\n",
            "i =  1600\n",
            "loss =  0.013974281027913094\n",
            "i =  1920\n",
            "loss =  0.0131632499396801\n",
            "i =  2240\n",
            "loss =  0.024017546325922012\n",
            "i =  2560\n",
            "loss =  0.007986421696841717\n",
            "evaluating...\n",
            "epochs: 6\n",
            "PAJO-AJ AUC =  0.9059818179889466\n",
            "Specificity =  0.8656716417910447\n",
            "Sensitivity =  0.8009950248756219\n",
            "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
            "[INFO] Register count_softmax() for <class 'torch.nn.modules.activation.Softmax'>.\n",
            "Flops: 696451625552.00\n",
            "MLOPS: 9.92\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9103    0.8657    0.8874       469\n",
            "           1     0.7188    0.8010    0.7576       201\n",
            "\n",
            "    accuracy                         0.8463       670\n",
            "   macro avg     0.8145    0.8333    0.8225       670\n",
            "weighted avg     0.8528    0.8463    0.8485       670\n",
            "\n",
            "i =  0\n",
            "loss =  0.012557084672152996\n",
            "i =  320\n",
            "loss =  0.0010309908539056778\n",
            "i =  640\n",
            "loss =  0.004178008530288935\n",
            "i =  960\n",
            "loss =  0.0009919115109369159\n",
            "i =  1280\n",
            "loss =  0.0005670094978995621\n",
            "i =  1600\n",
            "loss =  0.0026088897138834\n",
            "i =  1920\n",
            "loss =  0.007226171903312206\n",
            "i =  2240\n",
            "loss =  0.015340836718678474\n",
            "i =  2560\n",
            "loss =  0.004513642750680447\n",
            "i =  0\n",
            "loss =  0.010087374597787857\n",
            "i =  320\n",
            "loss =  0.01858554780483246\n",
            "i =  640\n",
            "loss =  0.007165655959397554\n",
            "i =  960\n",
            "loss =  0.012733896262943745\n",
            "i =  1280\n",
            "loss =  0.017953595146536827\n",
            "i =  1600\n",
            "loss =  0.0012308635050430894\n",
            "i =  1920\n",
            "loss =  0.0005943105788901448\n",
            "i =  2240\n",
            "loss =  0.03046274185180664\n",
            "i =  2560\n",
            "loss =  0.01514885388314724\n",
            "i =  0\n",
            "loss =  0.010660700500011444\n",
            "i =  320\n",
            "loss =  0.0024806582368910313\n",
            "i =  640\n",
            "loss =  0.009716631844639778\n",
            "i =  960\n",
            "loss =  0.0031063109636306763\n",
            "i =  1280\n",
            "loss =  0.0008236829307861626\n",
            "i =  1600\n",
            "loss =  0.03213860094547272\n",
            "i =  1920\n",
            "loss =  0.0025290746707469225\n",
            "i =  2240\n",
            "loss =  0.005739695858210325\n",
            "i =  2560\n",
            "loss =  0.0025197663344442844\n",
            "evaluating...\n",
            "epochs: 9\n",
            "PAJO-AJ AUC =  0.8984183559812875\n",
            "Specificity =  0.7590618336886994\n",
            "Sensitivity =  0.8507462686567164\n",
            "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
            "[INFO] Register count_softmax() for <class 'torch.nn.modules.activation.Softmax'>.\n",
            "Flops: 696451625552.00\n",
            "MLOPS: 9.89\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9223    0.7591    0.8327       469\n",
            "           1     0.6021    0.8507    0.7052       201\n",
            "\n",
            "    accuracy                         0.7866       670\n",
            "   macro avg     0.7622    0.8049    0.7690       670\n",
            "weighted avg     0.8262    0.7866    0.7945       670\n",
            "\n",
            "i =  0\n",
            "loss =  0.022090015932917595\n",
            "i =  320\n",
            "loss =  0.010927190072834492\n",
            "i =  640\n",
            "loss =  0.00402412423864007\n",
            "i =  960\n",
            "loss =  7.489402196370065e-05\n",
            "i =  1280\n",
            "loss =  0.0045555769465863705\n",
            "i =  1600\n",
            "loss =  0.0003542242047842592\n",
            "i =  1920\n",
            "loss =  0.001983100548386574\n",
            "i =  2240\n",
            "loss =  0.0004104221588931978\n",
            "i =  2560\n",
            "loss =  0.0001771013776306063\n",
            "i =  0\n",
            "loss =  0.0004723627644125372\n",
            "i =  320\n",
            "loss =  0.010213363915681839\n",
            "i =  640\n",
            "loss =  0.004155728500336409\n",
            "i =  960\n",
            "loss =  0.0010751505615189672\n",
            "i =  1280\n",
            "loss =  0.0001605645811650902\n",
            "i =  1600\n",
            "loss =  0.0037271229084581137\n",
            "i =  1920\n",
            "loss =  0.005225388798862696\n",
            "i =  2240\n",
            "loss =  0.0005585704930126667\n",
            "i =  2560\n",
            "loss =  0.00017284133355133235\n",
            "i =  0\n",
            "loss =  0.001713938545435667\n",
            "i =  320\n",
            "loss =  3.288922380306758e-05\n",
            "i =  640\n",
            "loss =  0.003545851446688175\n",
            "i =  960\n",
            "loss =  0.006417829543352127\n",
            "i =  1280\n",
            "loss =  8.558812260162085e-05\n",
            "i =  1600\n",
            "loss =  0.00012418636470101774\n",
            "i =  1920\n",
            "loss =  1.1322881618980318e-05\n",
            "i =  2240\n",
            "loss =  9.79788601398468e-05\n",
            "i =  2560\n",
            "loss =  6.926461355760694e-05\n",
            "evaluating...\n",
            "epochs: 12\n",
            "PAJO-AJ AUC =  0.8948010480645812\n",
            "Specificity =  0.9147121535181236\n",
            "Sensitivity =  0.6616915422885572\n",
            "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
            "[INFO] Register count_softmax() for <class 'torch.nn.modules.activation.Softmax'>.\n",
            "Flops: 696451625552.00\n",
            "MLOPS: 9.89\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8632    0.9147    0.8882       469\n",
            "           1     0.7688    0.6617    0.7112       201\n",
            "\n",
            "    accuracy                         0.8388       670\n",
            "   macro avg     0.8160    0.7882    0.7997       670\n",
            "weighted avg     0.8349    0.8388    0.8351       670\n",
            "\n",
            "i =  0\n",
            "loss =  0.011291522532701492\n",
            "i =  320\n",
            "loss =  0.0002907683083321899\n",
            "i =  640\n",
            "loss =  4.533439550868934e-06\n",
            "i =  960\n",
            "loss =  5.980011337669566e-06\n",
            "i =  1280\n",
            "loss =  1.0813885637617204e-05\n",
            "i =  1600\n",
            "loss =  0.002558105858042836\n",
            "i =  1920\n",
            "loss =  1.9592271200963296e-05\n",
            "i =  2240\n",
            "loss =  0.030164537951350212\n",
            "i =  2560\n",
            "loss =  0.0003442264278419316\n",
            "i =  0\n",
            "loss =  0.0036367131397128105\n",
            "i =  320\n",
            "loss =  0.0012941008899360895\n",
            "i =  640\n",
            "loss =  0.005909031257033348\n",
            "i =  960\n",
            "loss =  0.002899599028751254\n",
            "i =  1280\n",
            "loss =  0.00016714405501261353\n",
            "i =  1600\n",
            "loss =  0.0024372171610593796\n",
            "i =  1920\n",
            "loss =  8.309340955747757e-06\n",
            "i =  2240\n",
            "loss =  3.89174310839735e-05\n",
            "i =  2560\n",
            "loss =  0.0006142029887996614\n",
            "i =  0\n",
            "loss =  0.00016408406372647732\n",
            "i =  320\n",
            "loss =  0.0029659667052328587\n",
            "i =  640\n",
            "loss =  0.0033875219523906708\n",
            "i =  960\n",
            "loss =  0.0013046507956460118\n",
            "i =  1280\n",
            "loss =  0.00036313553573563695\n",
            "i =  1600\n",
            "loss =  0.004299671854823828\n",
            "i =  1920\n",
            "loss =  0.0012048346688970923\n",
            "i =  2240\n",
            "loss =  0.00909237191081047\n",
            "i =  2560\n",
            "loss =  0.0025614160113036633\n",
            "evaluating...\n",
            "epochs: 15\n",
            "PAJO-AJ AUC =  0.892679459843639\n",
            "Specificity =  0.929637526652452\n",
            "Sensitivity =  0.5572139303482587\n",
            "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
            "[INFO] Register count_softmax() for <class 'torch.nn.modules.activation.Softmax'>.\n",
            "Flops: 696451625552.00\n",
            "MLOPS: 9.88\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8305    0.9296    0.8773       469\n",
            "           1     0.7724    0.5572    0.6474       201\n",
            "\n",
            "    accuracy                         0.8179       670\n",
            "   macro avg     0.8014    0.7434    0.7623       670\n",
            "weighted avg     0.8131    0.8179    0.8083       670\n",
            "\n",
            "i =  0\n",
            "loss =  0.001000637887045741\n",
            "i =  320\n",
            "loss =  0.0003703943220898509\n",
            "i =  640\n",
            "loss =  1.8809087123372592e-05\n",
            "i =  960\n",
            "loss =  0.0020977831445634365\n",
            "i =  1280\n",
            "loss =  5.717940803151578e-05\n",
            "i =  1600\n",
            "loss =  2.1318519429769367e-05\n",
            "i =  1920\n",
            "loss =  1.4363388800120447e-05\n",
            "i =  2240\n",
            "loss =  0.0003436077677179128\n",
            "i =  2560\n",
            "loss =  0.004119185730814934\n",
            "i =  0\n",
            "loss =  0.00010392319381935522\n",
            "i =  320\n",
            "loss =  0.003967336378991604\n",
            "i =  640\n",
            "loss =  0.001879054238088429\n",
            "i =  960\n",
            "loss =  6.189577106852084e-05\n",
            "i =  1280\n",
            "loss =  0.0015049049397930503\n",
            "i =  1600\n",
            "loss =  5.7284611102659255e-05\n",
            "i =  1920\n",
            "loss =  2.374512860114919e-06\n",
            "i =  2240\n",
            "loss =  0.0012752217007800937\n",
            "i =  2560\n",
            "loss =  3.79845096176723e-06\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from thop import profile\n",
        "import time\n",
        "\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "# device = 'cpu'\n",
        "\n",
        "batch_size = 16\n",
        "lr = 3e-5\n",
        "EPOCHS = 18\n",
        "\n",
        "def shuffle_dataset(journal, title, abstruct, label):\n",
        "    length = len(journal)\n",
        "    rng = np.random.default_rng(12345)\n",
        "    index = np.arange(length)\n",
        "    # print(index)\n",
        "    rng.shuffle(index)\n",
        "    # print(index)\n",
        "    return journal[index], title[index], abstruct[index], label[index]\n",
        "\n",
        "def evaluate(model, test_journal, test_title, test_abstruct, test_Y):\n",
        "    model.eval()\n",
        "    pred = []\n",
        "    logits = []\n",
        "    true_Y = []\n",
        "    length = len(test_Y)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(test_Y), batch_size):\n",
        "        # for i in range(0, 10, batch_size):\n",
        "            logit = model(test_journal[i:min(len(test_Y),i+batch_size)], test_title[i:min(len(test_Y),i+batch_size)], test_abstruct[i:min(len(test_Y),i+batch_size)]).cpu()\n",
        "            logits.extend(logit)\n",
        "            y_pred = torch.argmax(logit, dim=1).cpu()\n",
        "            pred.extend(y_pred)\n",
        "            true_Y.extend(test_Y[i:min(len(test_Y),i+batch_size)].cpu())\n",
        "        # print(true_Y)\n",
        "        # print(logits)\n",
        "        # print(pred)\n",
        "        true_label = []\n",
        "        prob = []\n",
        "        pred_label = []\n",
        "        for i in range(len(true_Y)):\n",
        "          true_label.append(true_Y[i].item())\n",
        "          prob.append(logits[i][1].item())\n",
        "          pred_label.append(pred[i].item())\n",
        "        # print(true_label)\n",
        "        # print(prob)\n",
        "        # print(pred_label)\n",
        "\n",
        "        # 计算specificity、sensitivity\n",
        "        tn, fp, fn, tp = confusion_matrix(true_label, pred_label).ravel()\n",
        "        specificity = tn / (tn + fp)\n",
        "        sensitivity = tp / (tp + fn)\n",
        "\n",
        "    return {\n",
        "        'label':true_label, 'proba':prob,\n",
        "        'AUC':roc_auc_score(true_label, prob),\n",
        "        'classification_report':classification_report(true_label, pred_label, digits=4),\n",
        "        'specificity': specificity,\n",
        "        'sensitivity': sensitivity\n",
        "    }\n",
        "\n",
        "\n",
        "def main():\n",
        "    # 按0和1生成训练集和测试集\n",
        "    X_0 = pd.read_excel('/content/drive/MyDrive/pajo_data/X_0.xlsx').values\n",
        "    Y_0 = pd.read_excel('/content/drive/MyDrive/pajo_data/Y_0.xlsx').values\n",
        "    X_1 = pd.read_excel('/content/drive/MyDrive/pajo_data/X_1.xlsx').values\n",
        "    Y_1 = pd.read_excel('/content/drive/MyDrive/pajo_data/Y_1.xlsx').values\n",
        "    title_0 = np.load('/content/drive/MyDrive/pajo_data/token_title_0.npy')\n",
        "    title_1 = np.load('/content/drive/MyDrive/pajo_data/token_title_1.npy')\n",
        "    abstruct_0 = np.load('/content/drive/MyDrive/pajo_data/token_abstruct_0_512.npy')\n",
        "    abstruct_1 = np.load('/content/drive/MyDrive/pajo_data/token_abstruct_1_512.npy')\n",
        "\n",
        "\n",
        "    rs = np.random.RandomState(42)\n",
        "    L = list(rs.randint(0, len(X_0), int(7/3*len(X_1))))\n",
        "    X_0 = X_0[L]\n",
        "    Y_0 = Y_0[L]\n",
        "    title_0=title_0[L]\n",
        "    abstruct_0 = abstruct_0[L]\n",
        "\n",
        "    journal_train_X_0, journal_test_X_0, train_Y_0,test_Y_0 = train_test_split(X_0, Y_0, train_size=0.80, random_state=42)\n",
        "    journal_train_X_1, journal_test_X_1, train_Y_1, test_Y_1 = train_test_split(X_1, Y_1, train_size=0.80, random_state=42)\n",
        "    title_train_X_0, title_test_X_0, _,_ = train_test_split(title_0, Y_0, train_size=0.80, random_state=42)\n",
        "    title_train_X_1, title_test_X_1, _,_ = train_test_split(title_1, Y_1, train_size=0.80, random_state=42)\n",
        "    abstruct_train_X_0, abstruct_test_X_0, _,_ = train_test_split(abstruct_0, Y_0, train_size=0.80, random_state=42)\n",
        "    abstruct_train_X_1, abstruct_test_X_1, _,_ = train_test_split(abstruct_1, Y_1, train_size=0.80, random_state=42)\n",
        "\n",
        "    test_journal = torch.from_numpy(np.vstack((journal_test_X_1, journal_test_X_0))).float().to(device)\n",
        "    test_title = torch.from_numpy(np.vstack((title_test_X_1, title_test_X_0))).to(device)\n",
        "    test_abstruct = torch.from_numpy(np.vstack((abstruct_test_X_1, abstruct_test_X_0))).to(device)\n",
        "    test_Y = torch.from_numpy(np.vstack((test_Y_1, test_Y_0))).to(device)\n",
        "    test_journal, test_title, test_abstruct, test_Y = shuffle_dataset(test_journal, test_title, test_abstruct, test_Y)\n",
        "\n",
        "\n",
        "    model = MyModel(embed_dim=768, journal_size=test_journal.shape[1]).to(device)\n",
        "#     model.load_state_dict(torch.load(\"./res/new_model_2_0.75.pt\"))\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "    journal_train_X_0 = torch.from_numpy(journal_train_X_0).float().to(device)\n",
        "    title_train_X_0 = torch.from_numpy(title_train_X_0).to(device)\n",
        "    abstruct_train_X_0 = torch.from_numpy(abstruct_train_X_0).to(device)\n",
        "    train_Y_0 = torch.from_numpy(train_Y_0).to(device)\n",
        "\n",
        "    journal_train_X_1 = torch.from_numpy(journal_train_X_1).float().float().to(device)\n",
        "    title_train_X_1 = torch.from_numpy(title_train_X_1).to(device)\n",
        "    abstruct_train_X_1 = torch.from_numpy(abstruct_train_X_1).to(device)\n",
        "    train_Y_1 = torch.from_numpy(train_Y_1).to(device)\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        start_time = time.time()\n",
        "        journal_train_X_0, title_train_X_0, abstruct_train_X_0, train_Y_0 = shuffle_dataset(journal_train_X_0, title_train_X_0, abstruct_train_X_0, train_Y_0)\n",
        "        train_journal = torch.cat((journal_train_X_1, journal_train_X_0))\n",
        "        # print('journal_train_X_1.shape:',journal_train_X_1.shape)\n",
        "        train_title = torch.cat((title_train_X_1, title_train_X_0))\n",
        "        train_abstruct = torch.cat((abstruct_train_X_1, abstruct_train_X_0))\n",
        "        train_Y = torch.cat((train_Y_1, train_Y_0))\n",
        "        train_journal, train_title, train_abstruct, train_Y = shuffle_dataset(train_journal, train_title, train_abstruct, train_Y)\n",
        "\n",
        "        for i in range(0, len(train_Y), batch_size):\n",
        "        # for i in range(0, 10, batch_size):\n",
        "            model.zero_grad()\n",
        "            logits = model(train_journal[i:min(len(train_Y),i+batch_size)], train_title[i:min(len(train_Y),i+batch_size)], train_abstruct[i:min(len(train_Y),i+batch_size)])\n",
        "            loss = focal_loss(logits[:,1].unsqueeze(1), train_Y[i:i+batch_size])\n",
        "            loss.backward()\n",
        "            if((i/batch_size)%20 == 0):\n",
        "              print('i = ', i)\n",
        "              print('loss = ', loss.item())\n",
        "            optimizer.step()\n",
        "\n",
        "        end_time = time.time()\n",
        "\n",
        "        # 计算每秒钟的训练样本数（即MLOPS）\n",
        "        num_samples = len(train_Y)\n",
        "        training_time = end_time - start_time\n",
        "        samples_per_second = num_samples / training_time\n",
        "\n",
        "        if(epoch%3 == 0):\n",
        "\n",
        "              print('evaluating...')\n",
        "\n",
        "              val_metrics = evaluate(model, test_journal, test_title, test_abstruct, test_Y)\n",
        "\n",
        "              print('epochs:',epoch)\n",
        "              print('PAJO-AJ AUC = ',val_metrics['AUC'])\n",
        "              print('Specificity = ', val_metrics['specificity'])\n",
        "              print('Sensitivity = ', val_metrics['sensitivity'])\n",
        "              # 输出模型信息\n",
        "              flops, params = profile(model, inputs=(train_journal[0:batch_size], train_title[0:batch_size], train_abstruct[0:batch_size]))\n",
        "              print(\"Flops: {:.2f}\".format(flops))\n",
        "              print(\"MLOPS: {:.2f}\".format(samples_per_second))\n",
        "              print(val_metrics['classification_report'])\n",
        "\n",
        "              torch.save(model.state_dict(), './res/abs_jou_pumbed_2_0.80_{}.pt'.format(epoch))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    if not os.path.exists('./res'):\n",
        "        os.mkdir('./res')\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dw2FFwb8Jm_"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings(action = 'ignore')\n",
        "\n",
        "def best_yuzhi_aimed_at_1(preda,y_test):\n",
        "    #preda为预测为1类的概率,输入形式为narray\n",
        "    precisions = []\n",
        "    recalls = []\n",
        "    for i in np.arange(0.01,1,0.01):\n",
        "        y_pred = np.where(preda<i,0,1)\n",
        "        TN,FP,FN,TP = metrics.confusion_matrix(y_test,y_pred).ravel()\n",
        "        precision = TP/(TP+FP)\n",
        "        recall = TP/(TP+FN)\n",
        "        precisions.append(precision)\n",
        "        recalls.append(recall)\n",
        "    precisions = np.array(precisions)\n",
        "    recalls = np.array(recalls)\n",
        "    f1_scores = (2 * precisions * recalls) / (precisions + recalls)\n",
        "    best_f1_score = np.max(f1_scores[np.isfinite(f1_scores)])\n",
        "    best_f1_score_index = np.argmax(f1_scores[np.isfinite(f1_scores)])\n",
        "    return best_f1_score, np.arange(0.01,1,0.01)[best_f1_score_index]\n",
        "#调用示例\n",
        "y_test=np.array([0,0,1,1,1,0,1,0,1,0,1,1])\n",
        "preda = np.array([0.2,0.3,0.4,0.44,0.45,0.56,0.3,0.1,0.7,0.9,0.13,0.5])\n",
        "print(best_yuzhi_aimed_at_1(preda,y_test))\n",
        "#输出\n",
        "(0.7777777777777778, 0.11)#（1类最佳f1值和对应的阈值）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3P8cGu3T8Jm_"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "def plot_picture(y_test,probas):\n",
        "    # y_test测试集\n",
        "    # probas预测概率\n",
        "    CVD = pd.DataFrame()\n",
        "    CVD['正样本'] = y_test\n",
        "    CVD['score'] = probas\n",
        "    CVD = CVD.sort_values(by='score')\n",
        "    cvd_risk = CVD.reset_index(drop=True)\n",
        "    print(cvd_risk)\n",
        "    H = len(cvd_risk)\n",
        "    h = int(H / 10)\n",
        "    cvd = []\n",
        "    risk = []\n",
        "    h0 = 0\n",
        "    risk_count = 0\n",
        "    cvd_count = 0\n",
        "    for i in range(len(cvd_risk)):\n",
        "        if h0 + h > i + 1:\n",
        "            risk_count = risk_count + cvd_risk.loc[i, \"score\"]\n",
        "            if cvd_risk.loc[i, \"正样本\"] == 1:\n",
        "                cvd_count = cvd_count + 1\n",
        "        else:\n",
        "            h0 = h0 + h\n",
        "            cvd.append(round(cvd_count / h, 3))\n",
        "            risk.append(round(risk_count / h, 3))\n",
        "            risk_count = 0\n",
        "            cvd_count = 0\n",
        "\n",
        "    labels = ['10', '9', '8', '7', '6', '5', '4', '3', '2', '1']\n",
        "    cvd.reverse()\n",
        "\n",
        "    x = np.arange(len(labels))  # the label locations\n",
        "    width = 0.8  # the width of the bars\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    rects1 = ax.bar(x, cvd, width, color='royalblue')\n",
        "    # rects2 = ax.bar(x + width / 2, risk, width, label='Estimated', color='indianred')\n",
        "\n",
        "    # Add some text for labels, title and custom x-axis tick labels, etc.\n",
        "    ax.set_ylabel('Proportion of Positive Samples')\n",
        "    ax.set_xlabel('Decile of Estimated Score')\n",
        "    # ax.set_title('Observed Vs Estimated')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(labels)\n",
        "    ax.legend()\n",
        "    plt.axhline(y=np.mean(cvd),  linestyle='--', color='black')\n",
        "    def autolabel(rects):\n",
        "        \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
        "        for rect in rects:\n",
        "            height = rect.get_height()\n",
        "            ax.annotate('{}'.format(height),\n",
        "                        xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                        xytext=(0, 2),  # 3 points vertical offset\n",
        "                        textcoords=\"offset points\",\n",
        "                        ha='center', va='bottom')\n",
        "\n",
        "    autolabel(rects1)\n",
        "    # autolabel(rects2)\n",
        "    fig.tight_layout()\n",
        "    plt.savefig(\"柱状图.png\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MHAK3MV8JnA"
      },
      "outputs": [],
      "source": [
        "def yuzhi(preda,door=0.1):\n",
        "    predict=[]\n",
        "    for i in range(len(preda)):\n",
        "        if preda[i] < door:\n",
        "            predict.append(0)\n",
        "        else:\n",
        "            predict.append(1)\n",
        "    return predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNy4tcKt8JnA"
      },
      "outputs": [],
      "source": [
        "#按阈值最大\n",
        "from sklearn.metrics import roc_auc_score,roc_curve,auc\n",
        "from numpy import argmax\n",
        "def find_optimal_cutoff(tpr,fpr,threshold):\n",
        "    optimal_idx = np.argmax(tpr - fpr)\n",
        "    optimal_threshold = threshold[optimal_idx]\n",
        "    return optimal_threshold\n",
        "\n",
        "def best_confusion_matrix(y_test, y_test_predprob):\n",
        "    \"\"\"\n",
        "        根据真实值和预测值（预测概率）的向量来计算混淆矩阵和最优的划分阈值\n",
        "\n",
        "        Args:\n",
        "            y_test:真实值\n",
        "            y_test_predprob：预测值\n",
        "\n",
        "        Returns:\n",
        "            返回最佳划分阈值和混淆矩阵\n",
        "        \"\"\"\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, y_test_predprob, pos_label=1)\n",
        "    cutoff = find_optimal_cutoff(tpr,fpr,thresholds)\n",
        "    y_pred = yuzhi(y_test_predprob,cutoff)\n",
        "    print(metrics.confusion_matrix(y_true=y_test, y_pred=y_pred))\n",
        "    print(metrics.classification_report(y_true=y_test, y_pred=y_pred))\n",
        "    TN,FP,FN,TP = metrics.confusion_matrix(y_test,y_pred).ravel()\n",
        "    return cutoff,TN,FN,FP,TP\n",
        "best_confusion_matrix(test_Y_input,preda)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZdQM21f8JnA"
      },
      "outputs": [],
      "source": [
        "precisions, recalls, thresholds = precision_recall_curve(test_Y_input,predict)\n",
        "\n",
        "# 拿到最优结果以及索引\n",
        "f1_scores = (2 * precisions * recalls) / (precisions + recalls)\n",
        "best_f1_score = np.max(f1_scores[np.isfinite(f1_scores)])\n",
        "best_f1_score_index = np.argmax(f1_scores[np.isfinite(f1_scores)])\n",
        "\n",
        "# 阈值\n",
        "best_f1_score, thresholds[best_f1_score_index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-4S-nk4w61b",
        "outputId": "b72699f0-3c33-4c5e-df9a-a37e7b051c80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  data.zip\n",
            "   creating: data/\n",
            "  inflating: data/token_abstruct_0_400.npy  \n",
            "  inflating: data/token_abstruct_1_400.npy  \n",
            "  inflating: data/token_title_0.npy  \n",
            "  inflating: data/token_title_1.npy  \n",
            "  inflating: data/X_0.xlsx           \n",
            "  inflating: data/X_1.xlsx           \n",
            "  inflating: data/Y_0.xlsx           \n",
            "  inflating: data/Y_1.xlsx           \n",
            "   creating: data/__pycache__/\n",
            "  inflating: data/__pycache__/dataset.cpython-37.pyc  \n"
          ]
        }
      ],
      "source": [
        "!unzip data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qw7zyYoiZiaI",
        "outputId": "49f393f2-2f40-4461-ffd7-5a869e7a6b79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch_metric_learning\n",
            "  Downloading pytorch_metric_learning-1.5.2-py3-none-any.whl (111 kB)\n",
            "\u001b[K     |████████████████████████████████| 111 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_metric_learning) (4.64.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_metric_learning) (1.21.6)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from pytorch_metric_learning) (0.13.1+cu113)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_metric_learning) (1.12.1+cu113)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pytorch_metric_learning) (1.0.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (4.1.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pytorch_metric_learning) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pytorch_metric_learning) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pytorch_metric_learning) (1.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->pytorch_metric_learning) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->pytorch_metric_learning) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->pytorch_metric_learning) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->pytorch_metric_learning) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->pytorch_metric_learning) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->pytorch_metric_learning) (3.0.4)\n",
            "Installing collected packages: pytorch-metric-learning\n",
            "Successfully installed pytorch-metric-learning-1.5.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch_metric_learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0nsOF6V8Vf3",
        "outputId": "e93848d5-fa12-4bd9-8d8a-32241c5fe381"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.21.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqYGm62_8a4-",
        "outputId": "d01f4b10-31bb-4d30-f002-b7c1c59424c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.21.1-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 5.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 47.9 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 47.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 12.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.21.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZ4E0YDZZXh1",
        "outputId": "257db48b-6628-40a4-d2fd-dc09b3cf881c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  data.zip\n",
            "   creating: data/\n",
            "  inflating: data/token_abstruct_0_400.npy  \n",
            "  inflating: data/token_abstruct_1_400.npy  \n",
            "  inflating: data/token_title_0.npy  \n",
            "  inflating: data/token_title_1.npy  \n",
            "  inflating: data/X_0.xlsx           \n",
            "  inflating: data/X_1.xlsx           \n",
            "  inflating: data/Y_0.xlsx           \n",
            "  inflating: data/Y_1.xlsx           \n",
            "   creating: data/__pycache__/\n",
            "  inflating: data/__pycache__/dataset.cpython-37.pyc  \n"
          ]
        }
      ],
      "source": [
        "!unzip data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUwm114yjvuD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}